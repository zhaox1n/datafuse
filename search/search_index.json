{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"api/config/","text":"Get the config of the Datafuse query server. Examples \u00b6 curl http://127.0.0.1:8080/v1/configs Config { log_level: \"INFO\", log_dir: \"./_logs\", num_cpus: 16, mysql_handler_host: \"127.0.0.1\", mysql_handler_port: 3307, max_active_sessions: 256, clickhouse_handler_host: \"127.0.0.1\", clickhouse_handler_port: 9000, flight_api_address: \"127.0.0.1:9090\", http_api_address: \"127.0.0.1:8080\", metric_api_address: \"127.0.0.1:7070\", store_api_address: \"127.0.0.1:9191\", store_api_username: ******, store_api_password: ******, config_file: \"\" }","title":"Config"},{"location":"api/config/#examples","text":"curl http://127.0.0.1:8080/v1/configs Config { log_level: \"INFO\", log_dir: \"./_logs\", num_cpus: 16, mysql_handler_host: \"127.0.0.1\", mysql_handler_port: 3307, max_active_sessions: 256, clickhouse_handler_host: \"127.0.0.1\", clickhouse_handler_port: 9000, flight_api_address: \"127.0.0.1:9090\", http_api_address: \"127.0.0.1:8080\", metric_api_address: \"127.0.0.1:7070\", store_api_address: \"127.0.0.1:9191\", store_api_username: ******, store_api_password: ******, config_file: \"\" }","title":"Examples"},{"location":"development/coding-guidelines/","text":"This document describes the coding guidelines for the Datafuse Rust codebase. Code formatting \u00b6 All code formatting is enforced with rustfmt with a project-specific configuration. Below is an example command: $ make fmt Code analysis \u00b6 Clippy is used to catch common mistakes and is run as a part of continuous integration. Before submitting your code for review, you can run lint: $ make lint Code documentation \u00b6 Any public fields, functions, and methods should be documented with Rustdoc . Please follow the conventions as detailed below for modules, structs, enums, and functions. The single line is used as a preview when navigating Rustdoc. As an example, see the 'Structs' and 'Enums' sections in the collections Rustdoc. /// [Single line] One line summary description /// /// [Longer description] Multiple lines, inline code /// examples, invariants, purpose, usage, etc. [ Attributes ] If attributes exist , add after Rustdoc Example below: /// Represents (x, y) of a 2-dimensional grid /// /// A line is defined by 2 instances. /// A plane is defined by 3 instances. #[repr(C)] struct Point { x : i32 , y : i32 , } Testing \u00b6 Unit tests $ make unit-test Stateless tests $ make stateless-test","title":"Coding Guideline"},{"location":"development/coding-guidelines/#code-formatting","text":"All code formatting is enforced with rustfmt with a project-specific configuration. Below is an example command: $ make fmt","title":"Code formatting"},{"location":"development/coding-guidelines/#code-analysis","text":"Clippy is used to catch common mistakes and is run as a part of continuous integration. Before submitting your code for review, you can run lint: $ make lint","title":"Code analysis"},{"location":"development/coding-guidelines/#code-documentation","text":"Any public fields, functions, and methods should be documented with Rustdoc . Please follow the conventions as detailed below for modules, structs, enums, and functions. The single line is used as a preview when navigating Rustdoc. As an example, see the 'Structs' and 'Enums' sections in the collections Rustdoc. /// [Single line] One line summary description /// /// [Longer description] Multiple lines, inline code /// examples, invariants, purpose, usage, etc. [ Attributes ] If attributes exist , add after Rustdoc Example below: /// Represents (x, y) of a 2-dimensional grid /// /// A line is defined by 2 instances. /// A plane is defined by 3 instances. #[repr(C)] struct Point { x : i32 , y : i32 , }","title":"Code documentation"},{"location":"development/coding-guidelines/#testing","text":"Unit tests $ make unit-test Stateless tests $ make stateless-test","title":"Testing"},{"location":"development/contributing/","text":"Contributing to Datafuse \u00b6 Datafuse is an open project, and you can contribute to it in many ways. You can help with ideas, code, or documentation. We appreciate any efforts that help us to make the project better. Our goal is to make contributing to the Datafuse project easy and transparent. Thank you. Notes Once the code been merged, your name will be stoned in the system.contributors table forever. SELECT * FROM system.contributors Contributing \u00b6 To contribute to Datafuse, ensure that you have the latest version of the codebase, run the following: $ git clone https://github.com/datafuselabs/datafuse $ cd datafuse $ make setup $ make test Coding Guidelines \u00b6 For detailed guidance on how to contribute to the codebase refer to Coding Guidelines . Documentation \u00b6 All developer documentation is published on the Datafuse developer site, datafuse.rs . Pull Requests \u00b6 To submit your pull request: Fork the datafuse repo and create your branch from master . Open an regular issue for binding the pull request. Submit a draft pull requests , tag your work in progress. If you have added code that should be tested, add unit tests. Verify and ensure that the test suites passes, make test . Make sure your code passes both linters, make lint . Change the status to \u201cReady for review\u201d. Watch out the replies from the @datafuse-bots , she will be your guide. Code of Conduct \u00b6 Please refer to the Code of Conduct , which describes the expectations for interactions within the community. Issues \u00b6 Datafuse uses GitHub issues to track bugs. Please include necessary information and instructions to reproduce your issue.","title":"Contributing"},{"location":"development/contributing/#contributing-to-datafuse","text":"Datafuse is an open project, and you can contribute to it in many ways. You can help with ideas, code, or documentation. We appreciate any efforts that help us to make the project better. Our goal is to make contributing to the Datafuse project easy and transparent. Thank you. Notes Once the code been merged, your name will be stoned in the system.contributors table forever. SELECT * FROM system.contributors","title":"Contributing to Datafuse"},{"location":"development/contributing/#contributing","text":"To contribute to Datafuse, ensure that you have the latest version of the codebase, run the following: $ git clone https://github.com/datafuselabs/datafuse $ cd datafuse $ make setup $ make test","title":"Contributing"},{"location":"development/contributing/#coding-guidelines","text":"For detailed guidance on how to contribute to the codebase refer to Coding Guidelines .","title":"Coding Guidelines"},{"location":"development/contributing/#documentation","text":"All developer documentation is published on the Datafuse developer site, datafuse.rs .","title":"Documentation"},{"location":"development/contributing/#pull-requests","text":"To submit your pull request: Fork the datafuse repo and create your branch from master . Open an regular issue for binding the pull request. Submit a draft pull requests , tag your work in progress. If you have added code that should be tested, add unit tests. Verify and ensure that the test suites passes, make test . Make sure your code passes both linters, make lint . Change the status to \u201cReady for review\u201d. Watch out the replies from the @datafuse-bots , she will be your guide.","title":"Pull Requests"},{"location":"development/contributing/#code-of-conduct","text":"Please refer to the Code of Conduct , which describes the expectations for interactions within the community.","title":"Code of Conduct"},{"location":"development/contributing/#issues","text":"Datafuse uses GitHub issues to track bugs. Please include necessary information and instructions to reproduce your issue.","title":"Issues"},{"location":"development/how-to-write-aggregate-functions/","text":"How to write aggregate functions \u00b6 Datafuse allows us to write custom aggregate functions through rust code. It's not an easy way because you need to be a rustacean first. Datafuse has a plan to support writing UDAFs in other languages(like js, web assembly) in the future. In this section we will talk about how to write aggregate functions in Datafuse. AggregateFunction trait introduction \u00b6 All aggregate functions implement AggregateFunction trait, and we register them into a global static factory named FactoryFuncRef , the factory is just an index map and the key is the name of the aggregate function. Note Function name in Datafuse is case-insensitive. pub trait AggregateFunction : fmt :: Display + Sync + Send { fn name ( & self ) -> & str ; fn return_type ( & self ) -> Result < DataType > ; fn nullable ( & self , _input_schema : & DataSchema ) -> Result < bool > ; fn init_state ( & self , place : StateAddr ); fn state_layout ( & self ) -> Layout ; fn accumulate ( & self , _place : StateAddr , _arrays : & [ Series ], _input_rows : usize ) -> Result < () > ; fn accumulate_keys ( & self , _places : & [ StateAddr ], _offset : usize , _arrays : & [ Series ], _input_rows : usize , ) -> Result < () > ; fn serialize ( & self , _place : StateAddr , _writer : & mut BytesMut ) -> Result < () > ; fn deserialize ( & self , _place : StateAddr , _reader : & mut & [ u8 ]) -> Result < () > ; fn merge ( & self , _place : StateAddr , _rhs : StateAddr ) -> Result < () > ; fn merge_result ( & self , _place : StateAddr ) -> Result < DataValue > ; } Understand the functions \u00b6 The function name indicates the name of this function, such as sum , min . The function return_type indicates the return type of the function, it may vary with different arguments, such as sum(int8) -> int64 , sum(uint8) -> uint64 , sum(float64) -> float64 . The function nullable indicates whether the return_type is nullable or not. Before we start to introduce the function init_state , let's ask a question first: what's aggregate function state? It indicates the temporary results of an aggregate function. Because an aggregate function accumulates data in columns block by block and there will be some intermediate results after the aggregation. Therefore, the state must be mergeable, serializable. For example, in the avg aggregate function, we can represent the state like: struct AggregateAvgState<T: BinarySer + BinaryDe> { pub value: T, pub count: u64, } The function init_state initializes the aggregate function state, we ensure the memory is already allocated, and we just need to initial the state with the initial value. The function state_layout indicates the memory layout of the state. The function accumulate is used in aggregation with a single batch, which means the whole block can be aggregated in a single state, no other keys. The SQL query, which applies aggregation without group-by columns, will hit this function. Noted that the argument _arrays is the function arguments, we can safely get the array by index without index bound check because we must validate the argument numbers and types in function constructor. The _input_rows is the rows of the current block, and it may be useful when the _arrays is empty, e.g., count() function. The function accumulate_keys is similar to accumulate, but we must take into consideration the keys and offsets, for which each key represents a unique memory address named place. The function serialize serializes state into binary. The function deserialize deserializes state from binary. The function merge , can be used to merge other state into current state. The function merge_result , can be used to represent the aggregate function state into one-row field. Example \u00b6 Let's take an example of aggregate function sum . It's declared as AggregateSumFunction<T, SumT> , we can accept varying integer types like UInt8Type , Int8Type . T and SumT is logic types which implement DFNumericType . e.g., T is UInt8Type and SumT must be UInt64Type . Also, we can dispatch it using macros by matching the types of the arguments. Take a look at the dispatch_numeric_types to understand the dispatch macros. The AggregateSumState will be struct AggregateSumState<T> { pub value: Option<T>, } The generic T is from SumT::Native , the Option<T> can return null if nothing is passed into this function. Let's take into the function accumulate_keys , because this is the only function that a little hard to understand in this case. The places is the memory address of the first state in this row, so we can get the address of AggregateSumState<T> using places[row] + offset , then using place.get::<AggregateSumState<SumT::Native>>() to get the value of the corresponding state. Since we already know the array type of this function, we can safely cast it to arrow's PrimitiveArray<T> , here we make two branches to reduce the branch prediction of CPU, null and no_null . In no_null case, we just iterate the array and apply the sum , this is good for compiler to optimize the codes into vectorized codes. Ok, this example is pretty easy. If you already read this, you may have the ability to write a new function. Refer to other examples \u00b6 As you see, adding a new aggregate function in Datafuse is not as hard as you think. Before you start to add one, please refer to other aggregate function examples, such as min , count , max , avg . Testing \u00b6 To be a good engineer, don't forget to test your codes, please add unit tests and stateless tests after you finish the new aggregate functions. Summary \u00b6 We welcome all community users to contribute more powerful functions to Datafuse. If you find any problems, feel free to open an issue in GitHub, we will use our best efforts to help you.","title":"How to write aggregate functions"},{"location":"development/how-to-write-aggregate-functions/#how-to-write-aggregate-functions","text":"Datafuse allows us to write custom aggregate functions through rust code. It's not an easy way because you need to be a rustacean first. Datafuse has a plan to support writing UDAFs in other languages(like js, web assembly) in the future. In this section we will talk about how to write aggregate functions in Datafuse.","title":"How to write aggregate functions"},{"location":"development/how-to-write-aggregate-functions/#aggregatefunction-trait-introduction","text":"All aggregate functions implement AggregateFunction trait, and we register them into a global static factory named FactoryFuncRef , the factory is just an index map and the key is the name of the aggregate function. Note Function name in Datafuse is case-insensitive. pub trait AggregateFunction : fmt :: Display + Sync + Send { fn name ( & self ) -> & str ; fn return_type ( & self ) -> Result < DataType > ; fn nullable ( & self , _input_schema : & DataSchema ) -> Result < bool > ; fn init_state ( & self , place : StateAddr ); fn state_layout ( & self ) -> Layout ; fn accumulate ( & self , _place : StateAddr , _arrays : & [ Series ], _input_rows : usize ) -> Result < () > ; fn accumulate_keys ( & self , _places : & [ StateAddr ], _offset : usize , _arrays : & [ Series ], _input_rows : usize , ) -> Result < () > ; fn serialize ( & self , _place : StateAddr , _writer : & mut BytesMut ) -> Result < () > ; fn deserialize ( & self , _place : StateAddr , _reader : & mut & [ u8 ]) -> Result < () > ; fn merge ( & self , _place : StateAddr , _rhs : StateAddr ) -> Result < () > ; fn merge_result ( & self , _place : StateAddr ) -> Result < DataValue > ; }","title":"AggregateFunction trait introduction"},{"location":"development/how-to-write-aggregate-functions/#understand-the-functions","text":"The function name indicates the name of this function, such as sum , min . The function return_type indicates the return type of the function, it may vary with different arguments, such as sum(int8) -> int64 , sum(uint8) -> uint64 , sum(float64) -> float64 . The function nullable indicates whether the return_type is nullable or not. Before we start to introduce the function init_state , let's ask a question first: what's aggregate function state? It indicates the temporary results of an aggregate function. Because an aggregate function accumulates data in columns block by block and there will be some intermediate results after the aggregation. Therefore, the state must be mergeable, serializable. For example, in the avg aggregate function, we can represent the state like: struct AggregateAvgState<T: BinarySer + BinaryDe> { pub value: T, pub count: u64, } The function init_state initializes the aggregate function state, we ensure the memory is already allocated, and we just need to initial the state with the initial value. The function state_layout indicates the memory layout of the state. The function accumulate is used in aggregation with a single batch, which means the whole block can be aggregated in a single state, no other keys. The SQL query, which applies aggregation without group-by columns, will hit this function. Noted that the argument _arrays is the function arguments, we can safely get the array by index without index bound check because we must validate the argument numbers and types in function constructor. The _input_rows is the rows of the current block, and it may be useful when the _arrays is empty, e.g., count() function. The function accumulate_keys is similar to accumulate, but we must take into consideration the keys and offsets, for which each key represents a unique memory address named place. The function serialize serializes state into binary. The function deserialize deserializes state from binary. The function merge , can be used to merge other state into current state. The function merge_result , can be used to represent the aggregate function state into one-row field.","title":"Understand the functions"},{"location":"development/how-to-write-aggregate-functions/#example","text":"Let's take an example of aggregate function sum . It's declared as AggregateSumFunction<T, SumT> , we can accept varying integer types like UInt8Type , Int8Type . T and SumT is logic types which implement DFNumericType . e.g., T is UInt8Type and SumT must be UInt64Type . Also, we can dispatch it using macros by matching the types of the arguments. Take a look at the dispatch_numeric_types to understand the dispatch macros. The AggregateSumState will be struct AggregateSumState<T> { pub value: Option<T>, } The generic T is from SumT::Native , the Option<T> can return null if nothing is passed into this function. Let's take into the function accumulate_keys , because this is the only function that a little hard to understand in this case. The places is the memory address of the first state in this row, so we can get the address of AggregateSumState<T> using places[row] + offset , then using place.get::<AggregateSumState<SumT::Native>>() to get the value of the corresponding state. Since we already know the array type of this function, we can safely cast it to arrow's PrimitiveArray<T> , here we make two branches to reduce the branch prediction of CPU, null and no_null . In no_null case, we just iterate the array and apply the sum , this is good for compiler to optimize the codes into vectorized codes. Ok, this example is pretty easy. If you already read this, you may have the ability to write a new function.","title":"Example"},{"location":"development/how-to-write-aggregate-functions/#refer-to-other-examples","text":"As you see, adding a new aggregate function in Datafuse is not as hard as you think. Before you start to add one, please refer to other aggregate function examples, such as min , count , max , avg .","title":"Refer to other examples"},{"location":"development/how-to-write-aggregate-functions/#testing","text":"To be a good engineer, don't forget to test your codes, please add unit tests and stateless tests after you finish the new aggregate functions.","title":"Testing"},{"location":"development/how-to-write-aggregate-functions/#summary","text":"We welcome all community users to contribute more powerful functions to Datafuse. If you find any problems, feel free to open an issue in GitHub, we will use our best efforts to help you.","title":"Summary"},{"location":"development/profiling/","text":"flame graph \u00b6 Open http://localhost:8080/debug/pprof/profile?seconds=5 in browser, Datafuse debug api will response the svg of the flame graph to the client. go pprof tool \u00b6 go tool pprof http://localhost:8080/debug/pprof/profile?seconds=20 Fetching profile over HTTP from http://localhost:8080/debug/pprof/profile?seconds=20 Saved profile in /home/bohu/pprof/pprof.cpu.007.pb.gz Type: cpu Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 5011, 100% of 5011 total Dropped 248 nodes (cum <= 25) Showing top 10 nodes out of 204 flat flat% sum% cum cum% 5011 100% 100% 5011 100% backtrace::backtrace::libunwind::trace 0 0% 100% 162 3.23% <&alloc::vec::Vec<T,A> as core::iter::traits::collect::IntoIterator>::into_iter 0 0% 100% 45 0.9% <&mut I as core::iter::traits::iterator::Iterator>::next 0 0% 100% 77 1.54% <[A] as core::slice::cmp::SlicePartialEq<B>>::equal 0 0% 100% 35 0.7% <[u8; 8] as ahash::convert::Convert<u64>>::convert 0 0% 100% 199 3.97% <[u8] as ahash::convert::ReadFromSlice>::read_last_u64 0 0% 100% 73 1.46% <[u8] as ahash::convert::ReadFromSlice>::read_last_u64::as_array 0 0% 100% 220 4.39% <[u8] as ahash::convert::ReadFromSlice>::read_u64 0 0% 100% 701 13.99% <ahash::fallback_hash::AHasher as core::hash::Hasher>::write 0 0% 100% 26 0.52% <ahash::random_state::RandomState as core::hash::BuildHasher>::build_hash Or go tool pprof -http=0.0.0.0:8081 /home/bohu/pprof/pprof.cpu.007.pb.gz","title":"Profling"},{"location":"development/profiling/#flame-graph","text":"Open http://localhost:8080/debug/pprof/profile?seconds=5 in browser, Datafuse debug api will response the svg of the flame graph to the client.","title":"flame graph"},{"location":"development/profiling/#go-pprof-tool","text":"go tool pprof http://localhost:8080/debug/pprof/profile?seconds=20 Fetching profile over HTTP from http://localhost:8080/debug/pprof/profile?seconds=20 Saved profile in /home/bohu/pprof/pprof.cpu.007.pb.gz Type: cpu Entering interactive mode (type \"help\" for commands, \"o\" for options) (pprof) top Showing nodes accounting for 5011, 100% of 5011 total Dropped 248 nodes (cum <= 25) Showing top 10 nodes out of 204 flat flat% sum% cum cum% 5011 100% 100% 5011 100% backtrace::backtrace::libunwind::trace 0 0% 100% 162 3.23% <&alloc::vec::Vec<T,A> as core::iter::traits::collect::IntoIterator>::into_iter 0 0% 100% 45 0.9% <&mut I as core::iter::traits::iterator::Iterator>::next 0 0% 100% 77 1.54% <[A] as core::slice::cmp::SlicePartialEq<B>>::equal 0 0% 100% 35 0.7% <[u8; 8] as ahash::convert::Convert<u64>>::convert 0 0% 100% 199 3.97% <[u8] as ahash::convert::ReadFromSlice>::read_last_u64 0 0% 100% 73 1.46% <[u8] as ahash::convert::ReadFromSlice>::read_last_u64::as_array 0 0% 100% 220 4.39% <[u8] as ahash::convert::ReadFromSlice>::read_u64 0 0% 100% 701 13.99% <ahash::fallback_hash::AHasher as core::hash::Hasher>::write 0 0% 100% 26 0.52% <ahash::random_state::RandomState as core::hash::BuildHasher>::build_hash Or go tool pprof -http=0.0.0.0:8081 /home/bohu/pprof/pprof.cpu.007.pb.gz","title":"go pprof tool"},{"location":"development/roadmap/","text":"Datafuse roadmap 2021. Notes Sync from the #476 Main tasks \u00b6 This is Datafuse roadmap 2021. 1. Query/Store task \u00b6 Task Status Release Target Comments Query/Store API track #745 PROGRESS v0.5 Store Service track #1154 PLANNING Cloud Re-architected track #1408 PROGRESS 2. Distributed Query task \u00b6 Task Status Release Target Comments Query cluster track #747 PROGRESS v0.5 @zhang2014 Functions track #758 PROGRESS @sundy-li Queries track #765 PROGRESS @zhang2014 3. Distributed Store task \u00b6 Task Status Release Target Comments Store track #271 PROGRESS v0.5 4. Observability task \u00b6 Task Status Release Target Comments Observability track #795 PROGRESS v0.5 @BohuTANG 5. Test infra task \u00b6 Task Status Release Target Comments Test Infra track #796 PROGRESS v0.4 @ZhiHanZ 6. RBAC task \u00b6 Task Status Release Target Comments RBAC track #894 PROGRESS v0.5 Access Control and Account Management 7. CBO task \u00b6 Task Status Release Target Comments Cost based optimizer(CBO) track #915 PLANNING Table statistics and CBO 8. Deployment task \u00b6 Task Status Release Target Comments datafuse cli #938 PROGRESS v0.5 All-in-one tool for setting up, managing with Datafuse Experimental and interns tasks \u00b6 Task Status Release Target Comments Join #559 PROGRESS v0.5 @leiysky Online Playgroud PLANNING User can try the demo on the datafuse.rs website Sharding PLANNING Store supports partition sharding Window functions PLANNING Limited support for transactions PLANNING Hash method in ClickHouse way #754 DONE Tuple functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/tuple-functions/ Array functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/array-functions/ Lambda functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/#higher-order-functions Compile aggregate functions(JIT) PLANNING Reference: https://github.com/ClickHouse/ClickHouse/pull/24789","title":"Roadmap"},{"location":"development/roadmap/#main-tasks","text":"This is Datafuse roadmap 2021.","title":"Main tasks"},{"location":"development/roadmap/#1-querystore-task","text":"Task Status Release Target Comments Query/Store API track #745 PROGRESS v0.5 Store Service track #1154 PLANNING Cloud Re-architected track #1408 PROGRESS","title":"1. Query/Store task"},{"location":"development/roadmap/#2-distributed-query-task","text":"Task Status Release Target Comments Query cluster track #747 PROGRESS v0.5 @zhang2014 Functions track #758 PROGRESS @sundy-li Queries track #765 PROGRESS @zhang2014","title":"2. Distributed Query task"},{"location":"development/roadmap/#3-distributed-store-task","text":"Task Status Release Target Comments Store track #271 PROGRESS v0.5","title":"3. Distributed Store task"},{"location":"development/roadmap/#4-observability-task","text":"Task Status Release Target Comments Observability track #795 PROGRESS v0.5 @BohuTANG","title":"4. Observability task"},{"location":"development/roadmap/#5-test-infra-task","text":"Task Status Release Target Comments Test Infra track #796 PROGRESS v0.4 @ZhiHanZ","title":"5. Test infra task"},{"location":"development/roadmap/#6-rbac-task","text":"Task Status Release Target Comments RBAC track #894 PROGRESS v0.5 Access Control and Account Management","title":"6. RBAC task"},{"location":"development/roadmap/#7-cbo-task","text":"Task Status Release Target Comments Cost based optimizer(CBO) track #915 PLANNING Table statistics and CBO","title":"7. CBO task"},{"location":"development/roadmap/#8-deployment-task","text":"Task Status Release Target Comments datafuse cli #938 PROGRESS v0.5 All-in-one tool for setting up, managing with Datafuse","title":"8. Deployment task"},{"location":"development/roadmap/#experimental-and-interns-tasks","text":"Task Status Release Target Comments Join #559 PROGRESS v0.5 @leiysky Online Playgroud PLANNING User can try the demo on the datafuse.rs website Sharding PLANNING Store supports partition sharding Window functions PLANNING Limited support for transactions PLANNING Hash method in ClickHouse way #754 DONE Tuple functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/tuple-functions/ Array functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/array-functions/ Lambda functions PLANNING Reference: https://clickhouse.tech/docs/en/sql-reference/functions/#higher-order-functions Compile aggregate functions(JIT) PLANNING Reference: https://github.com/ClickHouse/ClickHouse/pull/24789","title":"Experimental and interns tasks"},{"location":"development/tracing/","text":"Tracing In Datafuse \u00b6 Datafuse using Rust's tracing ecosystem tokio-tracing to do log and profile. Datafuse default log level is INFO . Enable Tracing \u00b6 QUERY_LOG_LEVEL=\"DEBUG\" ./datafuse-query If we want to track the execution of a query: set max_threads=1; select sum(number+1)+1 from numbers(10000) where number>0 group by number%3; Tracing log: Tracing [2021-06-10T08:40:36Z DEBUG clickhouse_srv::cmd] Got packet Query(QueryRequest { query_id: \"bac2b254-6245-4cae-910d-3e5e979c8b68\", client_info: QueryClientInfo { query_kind: 1, initial_user: \"\", initial_query_id: \"\", initial_address: \"0.0.0.0:0\", interface: 1, os_user: \"bohu\", client_hostname: \"thinkpad\", client_name: \"ClickHouse \", client_version_major: 21, client_version_minor: 4, client_version_patch: 6, client_revision: 54447, http_method: 0, http_user_agent: \"\", quota_key: \"\" }, stage: 2, compression: 1, query: \"select sum(number+1)+1 from numbers(10000) where number>0 group by number%3;\" }) Jun 10 16:40:36.131 DEBUG ThreadId(16) datafuse_query::sql::plan_parser: query=\"select sum(number+1)+1 from numbers(10000) where number>0 group by number%3;\" [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Plus [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 30 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"1\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Function(Function { name: ObjectName([Ident { value: \"sum\", quote_style: None }]), args: [Unnamed(BinaryOp { left: Identifier(Ident { value: \"number\", quote_style: None }), op: Plus, right: Value(Number(\"1\", false)) })], over: None, distinct: false }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Plus [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 30 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"1\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"from\", quote_style: None, keyword: FROM }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"from\", quote_style: None, keyword: FROM }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"10000\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Gt [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 20 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"0\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"group\", quote_style: None, keyword: GROUP }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"group\", quote_style: None, keyword: GROUP }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Mod [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 40 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"3\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() SemiColon [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() SemiColon [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 Jun 10 16:40:36.135 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: new Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: enter Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: new Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: enter Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: exit Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: close time.busy=2.65ms time.idle=457\u00b5s Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: exit Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: close time.busy=3.57ms time.idle=453\u00b5s Jun 10 16:40:36.140 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: new Jun 10 16:40:36.141 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: enter Jun 10 16:40:36.141 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: Before ProjectionPushDown Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.142 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: After ProjectionPushDown Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.142 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: Before Scatters Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.143 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: After Scatters Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: new Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: enter Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: exit Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: close time.busy=145\u00b5s time.idle=264\u00b5s Jun 10 16:40:36.144 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: new Jun 10 16:40:36.144 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: enter Jun 10 16:40:36.144 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: Received plan: Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: Pipeline: ProjectionTransform \u00d7 1 processor ExpressionTransform \u00d7 1 processor GroupByFinalTransform \u00d7 1 processor GroupByPartialTransform \u00d7 1 processor ExpressionTransform \u00d7 1 processor FilterTransform \u00d7 1 processor SourceTransform \u00d7 1 processor Jun 10 16:40:36.145 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: exit Jun 10 16:40:36.145 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: close time.busy=1.07ms time.idle=215\u00b5s Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_projection: execute... Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_final: execute... Jun 10 16:40:36.146 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_partial: execute... Jun 10 16:40:36.146 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_source: execute, table:system.numbers, is_remote:false... Jun 10 16:40:36.148 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_filter: execute... Jun 10 16:40:36.148 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_expression_executor: (filter executor) execute, actions: [Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"0\", value: 0 }), Function(ActionFunction { name: \"(number > 0)\", func_name: \">\", return_type: Boolean, is_aggregated: false, arg_names: [\"number\", \"0\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.150 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_filter: Filter cost: 1.678104ms Jun 10 16:40:36.150 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_expression_executor: (expression executor) execute, actions: [Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"3\", value: 3 }), Function(ActionFunction { name: \"(number % 3)\", func_name: \"%\", return_type: UInt64, is_aggregated: false, arg_names: [\"number\", \"3\"], arg_types: [UInt64, UInt64], arg_fields: [] }), Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"1\", value: 1 }), Function(ActionFunction { name: \"(number + 1)\", func_name: \"+\", return_type: UInt64, is_aggregated: false, arg_names: [\"number\", \"1\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.165 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_partial: Group by partial cost: 18.822193ms Jun 10 16:40:36.166 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_final: Group by final cost: 20.170851ms Jun 10 16:40:36.167 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: exit Jun 10 16:40:36.167 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: close time.busy=26.1ms time.idle=592\u00b5s Jun 10 16:40:36.167 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_expression_executor: (expression executor) execute, actions: [Input(ActionInput { name: \"sum((number + 1))\", return_type: UInt64 }), Constant(ActionConstant { name: \"1\", value: 1 }), Function(ActionFunction { name: \"(sum((number + 1)) + 1)\", func_name: \"+\", return_type: UInt64, is_aggregated: false, arg_names: [\"sum((number + 1))\", \"1\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.168 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_expression_executor: (projection executor) execute, actions: [Input(ActionInput { name: \"(sum((number + 1)) + 1)\", return_type: UInt64 })] Jun 10 16:40:36.168 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_projection: Projection cost: 241.864\u00b5s","title":"Tracing"},{"location":"development/tracing/#tracing-in-datafuse","text":"Datafuse using Rust's tracing ecosystem tokio-tracing to do log and profile. Datafuse default log level is INFO .","title":"Tracing In Datafuse"},{"location":"development/tracing/#enable-tracing","text":"QUERY_LOG_LEVEL=\"DEBUG\" ./datafuse-query If we want to track the execution of a query: set max_threads=1; select sum(number+1)+1 from numbers(10000) where number>0 group by number%3; Tracing log: Tracing [2021-06-10T08:40:36Z DEBUG clickhouse_srv::cmd] Got packet Query(QueryRequest { query_id: \"bac2b254-6245-4cae-910d-3e5e979c8b68\", client_info: QueryClientInfo { query_kind: 1, initial_user: \"\", initial_query_id: \"\", initial_address: \"0.0.0.0:0\", interface: 1, os_user: \"bohu\", client_hostname: \"thinkpad\", client_name: \"ClickHouse \", client_version_major: 21, client_version_minor: 4, client_version_patch: 6, client_revision: 54447, http_method: 0, http_user_agent: \"\", quota_key: \"\" }, stage: 2, compression: 1, query: \"select sum(number+1)+1 from numbers(10000) where number>0 group by number%3;\" }) Jun 10 16:40:36.131 DEBUG ThreadId(16) datafuse_query::sql::plan_parser: query=\"select sum(number+1)+1 from numbers(10000) where number>0 group by number%3;\" [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Plus [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 30 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"1\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Function(Function { name: ObjectName([Ident { value: \"sum\", quote_style: None }]), args: [Unnamed(BinaryOp { left: Identifier(Ident { value: \"number\", quote_style: None }), op: Plus, right: Value(Number(\"1\", false)) })], over: None, distinct: false }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Plus [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 30 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"1\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"from\", quote_style: None, keyword: FROM }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"from\", quote_style: None, keyword: FROM }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"10000\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() RParen [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Gt [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 20 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"0\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"group\", quote_style: None, keyword: GROUP }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Word(Word { value: \"group\", quote_style: None, keyword: GROUP }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Identifier(Ident { value: \"number\", quote_style: None }) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() Mod [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 40 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] parsing expr [2021-06-10T08:40:36Z DEBUG sqlparser::parser] prefix: Value(Number(\"3\", false)) [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() SemiColon [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 [2021-06-10T08:40:36Z DEBUG sqlparser::parser] get_next_precedence() SemiColon [2021-06-10T08:40:36Z DEBUG sqlparser::parser] next precedence: 0 Jun 10 16:40:36.135 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: new Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: enter Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: new Jun 10 16:40:36.136 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: enter Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: exit Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan:select_to_plan: datafuse_query::sql::plan_parser: close time.busy=2.65ms time.idle=457\u00b5s Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: exit Jun 10 16:40:36.139 INFO ThreadId(16) sql_statement_to_plan: datafuse_query::sql::plan_parser: close time.busy=3.57ms time.idle=453\u00b5s Jun 10 16:40:36.140 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: new Jun 10 16:40:36.141 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: enter Jun 10 16:40:36.141 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: Before ProjectionPushDown Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.142 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: After ProjectionPushDown Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.142 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: Before Scatters Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.143 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::optimizers::optimizer: After Scatters Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: new Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: enter Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: exit Jun 10 16:40:36.143 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:reschedule: datafuse_query::interpreters::plan_scheduler: close time.busy=145\u00b5s time.idle=264\u00b5s Jun 10 16:40:36.144 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: new Jun 10 16:40:36.144 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: enter Jun 10 16:40:36.144 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: Received plan: Projection: (sum((number + 1)) + 1):UInt64 Expression: (sum((number + 1)) + 1):UInt64 (Before Projection) AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum((number + 1))]] Expression: (number % 3):UInt8, (number + 1):UInt64 (Before GroupBy) Filter: (number > 0) ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10000, read_bytes: 80000] Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: Pipeline: ProjectionTransform \u00d7 1 processor ExpressionTransform \u00d7 1 processor GroupByFinalTransform \u00d7 1 processor GroupByPartialTransform \u00d7 1 processor ExpressionTransform \u00d7 1 processor FilterTransform \u00d7 1 processor SourceTransform \u00d7 1 processor Jun 10 16:40:36.145 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: exit Jun 10 16:40:36.145 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}:build: datafuse_query::pipelines::processors::pipeline_builder: close time.busy=1.07ms time.idle=215\u00b5s Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_projection: execute... Jun 10 16:40:36.145 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_final: execute... Jun 10 16:40:36.146 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_partial: execute... Jun 10 16:40:36.146 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_source: execute, table:system.numbers, is_remote:false... Jun 10 16:40:36.148 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_filter: execute... Jun 10 16:40:36.148 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_expression_executor: (filter executor) execute, actions: [Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"0\", value: 0 }), Function(ActionFunction { name: \"(number > 0)\", func_name: \">\", return_type: Boolean, is_aggregated: false, arg_names: [\"number\", \"0\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.150 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_filter: Filter cost: 1.678104ms Jun 10 16:40:36.150 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_expression_executor: (expression executor) execute, actions: [Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"3\", value: 3 }), Function(ActionFunction { name: \"(number % 3)\", func_name: \"%\", return_type: UInt64, is_aggregated: false, arg_names: [\"number\", \"3\"], arg_types: [UInt64, UInt64], arg_fields: [] }), Input(ActionInput { name: \"number\", return_type: UInt64 }), Constant(ActionConstant { name: \"1\", value: 1 }), Function(ActionFunction { name: \"(number + 1)\", func_name: \"+\", return_type: UInt64, is_aggregated: false, arg_names: [\"number\", \"1\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.165 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_partial: Group by partial cost: 18.822193ms Jun 10 16:40:36.166 DEBUG ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::pipelines::transforms::transform_groupby_final: Group by final cost: 20.170851ms Jun 10 16:40:36.167 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: exit Jun 10 16:40:36.167 INFO ThreadId(309) execute{ctx.id=\"1c651744-3e73-4b94-9df0-dc031b73c626\"}: datafuse_query::interpreters::interpreter_select: close time.busy=26.1ms time.idle=592\u00b5s Jun 10 16:40:36.167 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_expression_executor: (expression executor) execute, actions: [Input(ActionInput { name: \"sum((number + 1))\", return_type: UInt64 }), Constant(ActionConstant { name: \"1\", value: 1 }), Function(ActionFunction { name: \"(sum((number + 1)) + 1)\", func_name: \"+\", return_type: UInt64, is_aggregated: false, arg_names: [\"sum((number + 1))\", \"1\"], arg_types: [UInt64, UInt64], arg_fields: [] })] Jun 10 16:40:36.168 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_expression_executor: (projection executor) execute, actions: [Input(ActionInput { name: \"(sum((number + 1)) + 1)\", return_type: UInt64 })] Jun 10 16:40:36.168 DEBUG ThreadId(309) datafuse_query::pipelines::transforms::transform_projection: Projection cost: 241.864\u00b5s","title":"Enable Tracing"},{"location":"overview/architecture/","text":"Design Overview \u00b6 Datafuse is an open source elastic and scalable cloud warehouse, it offers blazing fast query and combines elasticity, simplicity, low cost of the cloud, built to make the Data Cloud easy. Datafuse is intended for executing workloads with data stored in cloud storage systems, such as AWS S3 and Azure Blob Storage or others. We design Datafuse with the following key functionalities in mind: Elastic In Datafuse storage and compute resources can be scaled in/out on demand. Secure All data files and network traffic in Datafuse is encrypted end-to-end, and provider Role Based Access Control in SQL level. User-friendly Datafuse is an ANSI SQL compliant cloud warehouse, it is easy for data scientist and engineers to use. Cost-efficient Datafuse processes queries with high performance, and the user only pays for what is actually used. The picture above shows the high-level architecture of Datafuse, it consists of three components: meta service layer, and the decoupled compute and storage layers. Meta Service Layer \u00b6 The meta service is a layer to service multiple tenants. This layer implements a persistent key-value store to store each tenant's state. In current implementation, the meta service has many components: Metadata, which manages all metadata of databases, tables, clusters, the transaction, etc. Administration, which stores user info, user management, access control information, usage statistics, etc. Security, which performs authorization and authentication to protect the privacy of users' data. The code of Meta Service Layer mainly resides in the store directory of the repository. Compute Layer \u00b6 The compute layer is the layer to carry out computation for query processing. This layer may consist of many clusters, and each cluster may consist of many nodes. Each node is a compute unit, and is a collection of components: Planner The query planner builds an execution plan from the user's SQL statement and represents the query with different types of relational operators (such as Projection , Filter , Limit , etc.). For example: datafuse :) EXPLAIN SELECT number + 1 FROM numbers_mt(10) WHERE number > 8 LIMIT 2 \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Limit: 2 \u2502 \u2502 Projection: (number + 1):UInt64 \u2502 \u2502 Expression: (number + 1):UInt64 (Before Projection) \u2502 \u2502 Filter: (number > 8) \u2502 \u2502 ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Optimizer A rule based optimizer, some rules like predicate push down or pruning of unused columns. Processors A vector-based query execution pipeline, which is build by planner instructions. Each pipeline executor is a processor(such as SourceTransform , FilterTransform , etc.), it has zero or more inputs and zero or more outputs, and connected as a pipeline, it also can be distributed on multiple nodes judged by your query workload. For example: datafuse :) EXPLAIN PIPELINE SELECT number + 1 FROM numbers_mt(10000) WHERE number > 8 LIMIT 2 \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 LimitTransform \u00d7 1 processor \u2502 \u2502 Merge (ProjectionTransform \u00d7 16 processors) to (LimitTransform \u00d7 1) \u2502 \u2502 ProjectionTransform \u00d7 16 processors \u2502 \u2502 ExpressionTransform \u00d7 16 processors \u2502 \u2502 FilterTransform \u00d7 16 processors \u2502 \u2502 SourceTransform \u00d7 16 processors \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Cache The cache utilizes local SSDs for caching Data and Indexes based on the version within a node. The cache can be warmed up with different strategies: LOAD_ON_DEMAND - Load index or table data on demand(By Default). LOAD_INDEXES - Load indexes only. LOAD_ALL - Load full data and indexes. Node is the smallest unit of the compute layer, they can be registered as one cluster via namespace. Many clusters can attach the same database, so they can serve the query in parallel by different users. The Compute Layer codes mainly in the query directory. Storage Layer \u00b6 Datafuse stores data in an efficient, columnar format as Parquet files. Each Parquet file is sorted by the primary key before being written to the underlying shared storage. For efficient pruning, Datafuse also creates indexes for each Parquet file: min_max.idx The index file stores the minimum and maximum value of this Parquet file. sparse.idx The index file store the mapping for every [N] records granularity. With the indexes, we can speed up the queries by reducing the I/O and CPU cost. Imagine that Parquet file f1 has min_max.idx of [3, 5) and Parquet file f2 has min_max.idx of [4, 6) in column x , if the query predicate is WHERE x < 4 , only f1 needs to be accessed and processed.","title":"Whitepapers"},{"location":"overview/architecture/#design-overview","text":"Datafuse is an open source elastic and scalable cloud warehouse, it offers blazing fast query and combines elasticity, simplicity, low cost of the cloud, built to make the Data Cloud easy. Datafuse is intended for executing workloads with data stored in cloud storage systems, such as AWS S3 and Azure Blob Storage or others. We design Datafuse with the following key functionalities in mind: Elastic In Datafuse storage and compute resources can be scaled in/out on demand. Secure All data files and network traffic in Datafuse is encrypted end-to-end, and provider Role Based Access Control in SQL level. User-friendly Datafuse is an ANSI SQL compliant cloud warehouse, it is easy for data scientist and engineers to use. Cost-efficient Datafuse processes queries with high performance, and the user only pays for what is actually used. The picture above shows the high-level architecture of Datafuse, it consists of three components: meta service layer, and the decoupled compute and storage layers.","title":"Design Overview"},{"location":"overview/architecture/#meta-service-layer","text":"The meta service is a layer to service multiple tenants. This layer implements a persistent key-value store to store each tenant's state. In current implementation, the meta service has many components: Metadata, which manages all metadata of databases, tables, clusters, the transaction, etc. Administration, which stores user info, user management, access control information, usage statistics, etc. Security, which performs authorization and authentication to protect the privacy of users' data. The code of Meta Service Layer mainly resides in the store directory of the repository.","title":"Meta Service Layer"},{"location":"overview/architecture/#compute-layer","text":"The compute layer is the layer to carry out computation for query processing. This layer may consist of many clusters, and each cluster may consist of many nodes. Each node is a compute unit, and is a collection of components: Planner The query planner builds an execution plan from the user's SQL statement and represents the query with different types of relational operators (such as Projection , Filter , Limit , etc.). For example: datafuse :) EXPLAIN SELECT number + 1 FROM numbers_mt(10) WHERE number > 8 LIMIT 2 \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 Limit: 2 \u2502 \u2502 Projection: (number + 1):UInt64 \u2502 \u2502 Expression: (number + 1):UInt64 (Before Projection) \u2502 \u2502 Filter: (number > 8) \u2502 \u2502 ReadDataSource: scan partitions: [1], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Optimizer A rule based optimizer, some rules like predicate push down or pruning of unused columns. Processors A vector-based query execution pipeline, which is build by planner instructions. Each pipeline executor is a processor(such as SourceTransform , FilterTransform , etc.), it has zero or more inputs and zero or more outputs, and connected as a pipeline, it also can be distributed on multiple nodes judged by your query workload. For example: datafuse :) EXPLAIN PIPELINE SELECT number + 1 FROM numbers_mt(10000) WHERE number > 8 LIMIT 2 \u250c\u2500explain\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 LimitTransform \u00d7 1 processor \u2502 \u2502 Merge (ProjectionTransform \u00d7 16 processors) to (LimitTransform \u00d7 1) \u2502 \u2502 ProjectionTransform \u00d7 16 processors \u2502 \u2502 ExpressionTransform \u00d7 16 processors \u2502 \u2502 FilterTransform \u00d7 16 processors \u2502 \u2502 SourceTransform \u00d7 16 processors \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 Cache The cache utilizes local SSDs for caching Data and Indexes based on the version within a node. The cache can be warmed up with different strategies: LOAD_ON_DEMAND - Load index or table data on demand(By Default). LOAD_INDEXES - Load indexes only. LOAD_ALL - Load full data and indexes. Node is the smallest unit of the compute layer, they can be registered as one cluster via namespace. Many clusters can attach the same database, so they can serve the query in parallel by different users. The Compute Layer codes mainly in the query directory.","title":"Compute Layer"},{"location":"overview/architecture/#storage-layer","text":"Datafuse stores data in an efficient, columnar format as Parquet files. Each Parquet file is sorted by the primary key before being written to the underlying shared storage. For efficient pruning, Datafuse also creates indexes for each Parquet file: min_max.idx The index file stores the minimum and maximum value of this Parquet file. sparse.idx The index file store the mapping for every [N] records granularity. With the indexes, we can speed up the queries by reducing the I/O and CPU cost. Imagine that Parquet file f1 has min_max.idx of [3, 5) and Parquet file f2 has min_max.idx of [4, 6) in column x , if the query predicate is WHERE x < 4 , only f1 needs to be accessed and processed.","title":"Storage Layer"},{"location":"overview/building-and-running/","text":"This document describes how to build and run DatafuseQuery as a distributed query engine. 1. Deploy \u00b6 Run with Docker(Recommended) $ docker pull datafuselabs/datafuse $ docker run --init --rm -p 3307:3307 datafuselabs/datafuse Release binary $ curl -fsS https://raw.githubusercontent.com/datafuselabs/datafuse/master/scripts/installer/install-datafuse.sh | bash From source $ git clone https://github.com/datafuselabs/datafuse.git $ cd datafuse $ make setup $ make run 2. Client \u00b6 MySQL Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ mysql -h127.0.0.1 -P3307 mysql> SELECT avg(number) FROM numbers(1000000000); +-------------+ | avg(number) | +-------------+ | 499999999.5 | +-------------+ 1 row in set (0.05 sec) ClickHouse Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ clickhouse client --host 0.0.0.0 --port 9001 datafuse :) SELECT avg(number) FROM numbers(1000000000); SELECT avg(number) FROM numbers(1000000000) Query id: 89e06fba-1d57-464d-bfb0-238df85a2e66 \u250c\u2500avg(number)\u2500\u2510 \u2502 499999999.5 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1 rows in set. Elapsed: 0.062 sec. Processed 1.00 billion rows, 8.01 GB (16.16 billion rows/s., 129.38 GB/s.)","title":"Installation"},{"location":"overview/building-and-running/#1-deploy","text":"Run with Docker(Recommended) $ docker pull datafuselabs/datafuse $ docker run --init --rm -p 3307:3307 datafuselabs/datafuse Release binary $ curl -fsS https://raw.githubusercontent.com/datafuselabs/datafuse/master/scripts/installer/install-datafuse.sh | bash From source $ git clone https://github.com/datafuselabs/datafuse.git $ cd datafuse $ make setup $ make run","title":"1. Deploy"},{"location":"overview/building-and-running/#2-client","text":"MySQL Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ mysql -h127.0.0.1 -P3307 mysql> SELECT avg(number) FROM numbers(1000000000); +-------------+ | avg(number) | +-------------+ | 499999999.5 | +-------------+ 1 row in set (0.05 sec) ClickHouse Client Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. $ clickhouse client --host 0.0.0.0 --port 9001 datafuse :) SELECT avg(number) FROM numbers(1000000000); SELECT avg(number) FROM numbers(1000000000) Query id: 89e06fba-1d57-464d-bfb0-238df85a2e66 \u250c\u2500avg(number)\u2500\u2510 \u2502 499999999.5 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 1 rows in set. Elapsed: 0.062 sec. Processed 1.00 billion rows, 8.01 GB (16.16 billion rows/s., 129.38 GB/s.)","title":"2. Client"},{"location":"overview/performance/","text":"Note Memory SIMD-Vector processing performance only Dataset: 100,000,000,000 (100 Billion) Hardware: AMD Ryzen 7 PRO 4750U, 8 CPU Cores, 16 Threads Rust: rustc 1.56.0-nightly (e3b1c12be 2021-08-02) Build with Link-time Optimization and Using CPU Specific Instructions Query DatafuseQuery (v0.4.76-nightly) SELECT avg(number) FROM numbers_mt(100000000000) 3.712 s. (26.94 billion rows/s., 215.52 GB/s.) SELECT sum(number) FROM numbers_mt(100000000000) 3.669 s. (27.26 billion rows/s., 218.07 GB/s.) SELECT min(number) FROM numbers_mt(100000000000) 4.498 s. (22.23 billion rows/s., 177.85 GB/s.) SELECT max(number) FROM numbers_mt(100000000000) 4.438 s. (22.53 billion rows/s., 180.25 GB/s.) SELECT count(number) FROM numbers_mt(100000000000) 2.125 s. (47.07 billion rows/s., 376.53 GB/s.) SELECT sum(number+number+number) FROM numbers_mt(100000000000) 17.169 s. (5.82 billion rows/s., 46.60 GB/s.) SELECT sum(number) / count(number) FROM numbers_mt(100000000000) 3.696 s. (27.06 billion rows/s., 216.45 GB/s.) SELECT sum(number) / count(number), max(number), min(number) FROM numbers_mt(100000000000) 8.348 s. (11.98 billion rows/s., 95.83 GB/s.) SELECT number FROM numbers_mt(10000000000) ORDER BY number DESC LIMIT 10 3.164 s. (3.16 billion rows/s., 25.28 GB/s.) SELECT max(number), sum(number) FROM numbers_mt(1000000000) GROUP BY number % 3, number % 4, number % 5 LIMIT 10 1.657 s. (603.62 million rows/s., 4.83 GB/s.) Notes DatafuseQuery system.numbers_mt is 16-way parallelism processing, gist 100,000,000,000 records on laptop show Experience 100 billion performance on your laptop, talk is cheap just bench it","title":"Performance"},{"location":"policies/cla/","text":"Datafuse Labs, Inc. \u00b6 Contributor License Agreement \u00b6 Thank you for your interest in the open source project(s) managed by Datafuse Labs, Inc. (\"Datafuse Labs\"). In order to clarify the intellectual property license granted with Contributions from any person or entity, Datafuse Labs must have a Contributor License Agreement (\"CLA\") on file that has been entered into by each contributor, indicating agreement to the license terms below. This license is for your protection as a contributor as well as the protection of Datafuse Labs and its other contributors and users; it does not change your rights to use your own Contributions for any other purpose. 1. Definitions. \"You\" (or \"Your\") shall mean the copyright owner or legal entity authorized by the copyright owner that is entering into this CLA with Datafuse Labs. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"Contribution\" shall mean any code, documentation or other original works of authorship, including any modifications or additions to an existing work, that are intentionally submitted by You to Datafuse Labs for inclusion in, or documentation of, any of the products owned or managed by Datafuse Labs (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Datafuse Labs or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Datafuse Labs for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\" 2. Grant of Copyright License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. 3. Grant of Patent License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) were submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that Your Contribution, or the Work to which You have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this CLA for that Contribution or Work shall terminate as of the date such litigation is filed. 4. Authority. You represent and warrant that You are legally entitled to grant the above license. If You are an individual and Your employer(s) has rights to intellectual property that You create that includes Your Contributions, You represent that You have received permission to make Contributions on behalf of that employer, that Your employer has waived such rights for Your Contributions to Datafuse Labs, or that Your employer has entered into a separate CLA with Datafuse Labs covering Your Contributions. If You are a Company,You represent further that each employee making a Contribution to Datafuse Labs under the Company's name is authorized to submit Contributions on behalf of the Company. 5. Original Works. You represent and warrant that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent and warrant that, to Your knowledge, none of Your Contributions infringe, violate, or misappropriate any third party intellectual property or other proprietary rights. 6. Disclaimer. You are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, EXCEPT FOR THE WARRANTIES SET FORTH ABOVE, YOU PROVIDE YOUR CONTRIBUTIONS ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. 7. Submissions on Behalf of Others. Should You wish to submit work that is not Your original creation, You may submit it to Datafuse Labs separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\". 8. Additional Facts/Circumstances. You agree to notify Datafuse Labs of any facts or circumstances of which You become aware that would make the above representations and warranties inaccurate in any respect. 9. Authorization. If You are entering into this CLA as a Company, You represent and warrant that the individual accepting this CLA is duly authorized to enter into this CLA on the Company's behalf.","title":"Contributor License Agreement"},{"location":"policies/cla/#datafuse-labs-inc","text":"","title":"Datafuse Labs, Inc."},{"location":"policies/cla/#contributor-license-agreement","text":"Thank you for your interest in the open source project(s) managed by Datafuse Labs, Inc. (\"Datafuse Labs\"). In order to clarify the intellectual property license granted with Contributions from any person or entity, Datafuse Labs must have a Contributor License Agreement (\"CLA\") on file that has been entered into by each contributor, indicating agreement to the license terms below. This license is for your protection as a contributor as well as the protection of Datafuse Labs and its other contributors and users; it does not change your rights to use your own Contributions for any other purpose. 1. Definitions. \"You\" (or \"Your\") shall mean the copyright owner or legal entity authorized by the copyright owner that is entering into this CLA with Datafuse Labs. For legal entities, the entity making a Contribution and all other entities that control, are controlled by, or are under common control with that entity are considered to be a single Contributor. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity. \"Contribution\" shall mean any code, documentation or other original works of authorship, including any modifications or additions to an existing work, that are intentionally submitted by You to Datafuse Labs for inclusion in, or documentation of, any of the products owned or managed by Datafuse Labs (the \"Work\"). For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to Datafuse Labs or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, Datafuse Labs for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by You as \"Not a Contribution.\" 2. Grant of Copyright License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works. 3. Grant of Patent License. Subject to the terms and conditions of this CLA, You hereby grant to Datafuse Labs and to recipients of software distributed by Datafuse Labs a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by You that are necessarily infringed by Your Contribution(s) alone or by combination of Your Contribution(s) with the Work to which such Contribution(s) were submitted. If any entity institutes patent litigation against You or any other entity (including a cross-claim or counterclaim in a lawsuit) alleging that Your Contribution, or the Work to which You have contributed, constitutes direct or contributory patent infringement, then any patent licenses granted to that entity under this CLA for that Contribution or Work shall terminate as of the date such litigation is filed. 4. Authority. You represent and warrant that You are legally entitled to grant the above license. If You are an individual and Your employer(s) has rights to intellectual property that You create that includes Your Contributions, You represent that You have received permission to make Contributions on behalf of that employer, that Your employer has waived such rights for Your Contributions to Datafuse Labs, or that Your employer has entered into a separate CLA with Datafuse Labs covering Your Contributions. If You are a Company,You represent further that each employee making a Contribution to Datafuse Labs under the Company's name is authorized to submit Contributions on behalf of the Company. 5. Original Works. You represent and warrant that each of Your Contributions is Your original creation (see section 7 for submissions on behalf of others). You represent and warrant that, to Your knowledge, none of Your Contributions infringe, violate, or misappropriate any third party intellectual property or other proprietary rights. 6. Disclaimer. You are not expected to provide support for Your Contributions, except to the extent You desire to provide support. You may provide support for free, for a fee, or not at all. UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING, EXCEPT FOR THE WARRANTIES SET FORTH ABOVE, YOU PROVIDE YOUR CONTRIBUTIONS ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED, INCLUDING, WITHOUT LIMITATION, ANY WARRANTIES OR CONDITIONS OF TITLE, NON-INFRINGEMENT, MERCHANTABILITY, OR FITNESS FOR A PARTICULAR PURPOSE. 7. Submissions on Behalf of Others. Should You wish to submit work that is not Your original creation, You may submit it to Datafuse Labs separately from any Contribution, identifying the complete details of its source and of any license or other restriction (including, but not limited to, related patents, trademarks, and license agreements) of which You are personally aware, and conspicuously marking the work as \"Submitted on behalf of a third-party: [named here]\". 8. Additional Facts/Circumstances. You agree to notify Datafuse Labs of any facts or circumstances of which You become aware that would make the above representations and warranties inaccurate in any respect. 9. Authorization. If You are entering into this CLA as a Company, You represent and warrant that the individual accepting this CLA is duly authorized to enter into this CLA on the Company's behalf.","title":"Contributor License Agreement"},{"location":"policies/code-of-conduct/","text":"Contributor Covenant Code of Conduct \u00b6 Our Pledge \u00b6 In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation. Our Standards \u00b6 Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting Our Responsibilities \u00b6 Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful. Scope \u00b6 This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers. Enforcement \u00b6 Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at opensource@datafuselabs.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership. Attribution \u00b6 This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Contributor Covenant Code of Conduct"},{"location":"policies/code-of-conduct/#contributor-covenant-code-of-conduct","text":"","title":"Contributor Covenant Code of Conduct"},{"location":"policies/code-of-conduct/#our-pledge","text":"In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.","title":"Our Pledge"},{"location":"policies/code-of-conduct/#our-standards","text":"Examples of behavior that contributes to creating a positive environment include: Using welcoming and inclusive language Being respectful of differing viewpoints and experiences Gracefully accepting constructive criticism Focusing on what is best for the community Showing empathy towards other community members Examples of unacceptable behavior by participants include: The use of sexualized language or imagery and unwelcome sexual attention or advances Trolling, insulting/derogatory comments, and personal or political attacks Public or private harassment Publishing others' private information, such as a physical or electronic address, without explicit permission Other conduct which could reasonably be considered inappropriate in a professional setting","title":"Our Standards"},{"location":"policies/code-of-conduct/#our-responsibilities","text":"Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior. Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.","title":"Our Responsibilities"},{"location":"policies/code-of-conduct/#scope","text":"This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.","title":"Scope"},{"location":"policies/code-of-conduct/#enforcement","text":"Instances of abusive, harassing, or otherwise unacceptable behavior may be reported by contacting the project team at opensource@datafuselabs.com . All complaints will be reviewed and investigated and will result in a response that is deemed necessary and appropriate to the circumstances. The project team is obligated to maintain confidentiality with regard to the reporter of an incident. Further details of specific enforcement policies may be posted separately. Project maintainers who do not follow or enforce the Code of Conduct in good faith may face temporary or permanent repercussions as determined by other members of the project's leadership.","title":"Enforcement"},{"location":"policies/code-of-conduct/#attribution","text":"This Code of Conduct is adapted from the Contributor Covenant , version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html For answers to common questions about this code of conduct, see https://www.contributor-covenant.org/faq","title":"Attribution"},{"location":"policies/notice/","text":"Datafuse includes some code from \u00b6 arrow/datafusion pola-rs/polars We include the text of the original license below: Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Datafuse includes some code from"},{"location":"policies/notice/#datafuse-includes-some-code-from","text":"arrow/datafusion pola-rs/polars We include the text of the original license below: Licensed to the Apache Software Foundation (ASF) under one or more contributor license agreements. See the NOTICE file distributed with this work for additional information regarding copyright ownership. The ASF licenses this file to you under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.","title":"Datafuse includes some code from"},{"location":"rfcs/cli/0001-cli-design/","text":"Proposal: Datafuse CLI Design Doc \u00b6 Background \u00b6 fusecli is a command-line tool for creating, listing, logging, deleting datafuse running instances on local or on cloud. It also supports to port forward webUI, monitoring dashboard on local and run SQL query from command line Goals \u00b6 Centralized way to manage running datafuse cluster on local or on k8s(Cloud) (start, delete, update, log) Manage and install release Instances on local machine Show different dashboards on local (prometheus, jaeger, query web UI (like querybook or superset)) Support to run query or load data through command line TLS Authentication support: support client side authentication, and also support to configure mTLS for managed datafuse instances Non Goals for now \u00b6 More detailed managements like manage schema, table etc Query Task visualization (list and show all query tasks on current cluster) RBAC tenant management(add a subcommand tenant is helpful, and is compatible with this design) Installation \u00b6 Use single line install script to install fusecli on local machine curl -fsS https://raw.githubusercontent.com/datafuselabs/datafuse/master/scripts/installer/install.sh | bash SubCommands \u00b6 Cluster Create \u00b6 Create, Configure and switch to a datafuse cluster using the following command: fusecli cluster create --profile = <datafuse profile> Support three kinds of profile in alpha stage, local: local profile will run standalone datafuse cluster on local(one running fuse-query instance and one running fuse-store instance) demo: install a standalone datafuse instance on cloud(k8s or maybe fargate in later stage) cluster: install datafuse cluster on cloud( through datafuse operator ) Support to use flags or yaml or toml files for deployment configuration setup For example: Run datafuse instance on local and setup the api address, version, tls for it fusecli cluster create --profile = local --set local.mysql_port = 3307 --set local.http_address = 127 .0.0.1:7070 --set local.version = v0.4.88-nightly --set local.tls_key = <key file location> --set local.tls_cert = <cert file location --set local.ca_cert = <ca cert location> Create and configure datafuse cluster through toml file or yaml file fusecli cluster create -f cluster_configuration.toml Cluster List \u00b6 List all clusters managed by the command line (Name with * in the cluster used in current session) fusecli cluster list | NAME | PROFILE | CLUSTER | STATUS | ADDRESS | TLS | | default ( * ) | local | local | RUNNING | localhost:3307 | disabled | | demo | demo | minikube | RUNNING | 172 .0.0.11:3307 | disabled | | production | cluster | GKE | RUNNING | 192 .12.1.1:3307 | enabled | Cluster View \u00b6 View datafuse components in current cluster For example: In cluster profile: fusecli cluster view | NAME | CLUSTER | COMPONENT | STATUS | TLS | | query-1 | GKE | datafuse-query | running | enabled | | query-2 | GKE | datafuse-query | running | enabled | | query-3 | GKE | datafuse-query | pending | enabled | | store-1 | GKE | datafuse-store | running | enabled | | store-2 | GKE | datafuse-store | running | enabled | Check on disk utilization fusecli cluster df | NAME | COMPONENT | USED | ALWAYABLE | LOCATION | | local-disk-1 | Block | 10Gi | 90Gi | /mnt/fuse-store | | s3-disk | Object | 100 Gi | 1000Gi | s3://bucket-1/mnt/fuse-store | Cluster delete \u00b6 For local profile, pids in running instances shall be killed, and for cluster profile, computing pods would be deleted, can add some flags to delete disk resources as well( RBAC needed) fusecli cluster delete Cluster log \u00b6 Show logs in current running instance show all fuse-query logs fusecli cluster log --component = query --all The command above would show all fuse-store logs fusecli cluster log --component = store --all The command above would show datafuse operator logs fusecli cluster log --component = operator Cluster add \u00b6 Add component to current storage The following command add two query nodes, each need 2 cpu resource fusecli cluster add --component query --num 2 --cpu 2 --namespace = <operator namespace> Cluster use \u00b6 Switch to another cluster The Command will switch to another cluster run on GKE cloud fusecli cluster use --name cloud-instance-1 --cluster GKE --kubeconfig ~/.kube/config --kubecontext gke-cloud-1 Cluster Check \u00b6 Check whether current configuration can be deployed on given cluster It will check on port availability, and storage resource availability for deployment fusecli cluster check --profile = local It will check on cloud resources, whether compute nodes could be scheduled on given cloud platform, whether TLS configured etc fusecli cluster check -f deploy.yaml Cluster Analyze \u00b6 Analyze and troubleshooting on given configuration, difference between analyze and check is that analyze is troubleshooting on a running cluster, and check mainly used for pre-fight check fusecli cluster analyze --profile = local Cluster Update \u00b6 Update cluster to a newer version fusecli cluster update v0.5.1-nightly Query \u00b6 Run query using selected client fusecli query 'SELECT * FROM TABLE1' --client = mysql","title":"DatafuseCLI Design"},{"location":"rfcs/cli/0001-cli-design/#proposal-datafuse-cli-design-doc","text":"","title":"Proposal: Datafuse CLI Design Doc"},{"location":"rfcs/cli/0001-cli-design/#background","text":"fusecli is a command-line tool for creating, listing, logging, deleting datafuse running instances on local or on cloud. It also supports to port forward webUI, monitoring dashboard on local and run SQL query from command line","title":"Background"},{"location":"rfcs/cli/0001-cli-design/#goals","text":"Centralized way to manage running datafuse cluster on local or on k8s(Cloud) (start, delete, update, log) Manage and install release Instances on local machine Show different dashboards on local (prometheus, jaeger, query web UI (like querybook or superset)) Support to run query or load data through command line TLS Authentication support: support client side authentication, and also support to configure mTLS for managed datafuse instances","title":"Goals"},{"location":"rfcs/cli/0001-cli-design/#non-goals-for-now","text":"More detailed managements like manage schema, table etc Query Task visualization (list and show all query tasks on current cluster) RBAC tenant management(add a subcommand tenant is helpful, and is compatible with this design)","title":"Non Goals for now"},{"location":"rfcs/cli/0001-cli-design/#installation","text":"Use single line install script to install fusecli on local machine curl -fsS https://raw.githubusercontent.com/datafuselabs/datafuse/master/scripts/installer/install.sh | bash","title":"Installation"},{"location":"rfcs/cli/0001-cli-design/#subcommands","text":"","title":"SubCommands"},{"location":"rfcs/cli/0001-cli-design/#cluster-create","text":"Create, Configure and switch to a datafuse cluster using the following command: fusecli cluster create --profile = <datafuse profile> Support three kinds of profile in alpha stage, local: local profile will run standalone datafuse cluster on local(one running fuse-query instance and one running fuse-store instance) demo: install a standalone datafuse instance on cloud(k8s or maybe fargate in later stage) cluster: install datafuse cluster on cloud( through datafuse operator ) Support to use flags or yaml or toml files for deployment configuration setup For example: Run datafuse instance on local and setup the api address, version, tls for it fusecli cluster create --profile = local --set local.mysql_port = 3307 --set local.http_address = 127 .0.0.1:7070 --set local.version = v0.4.88-nightly --set local.tls_key = <key file location> --set local.tls_cert = <cert file location --set local.ca_cert = <ca cert location> Create and configure datafuse cluster through toml file or yaml file fusecli cluster create -f cluster_configuration.toml","title":"Cluster Create"},{"location":"rfcs/cli/0001-cli-design/#cluster-list","text":"List all clusters managed by the command line (Name with * in the cluster used in current session) fusecli cluster list | NAME | PROFILE | CLUSTER | STATUS | ADDRESS | TLS | | default ( * ) | local | local | RUNNING | localhost:3307 | disabled | | demo | demo | minikube | RUNNING | 172 .0.0.11:3307 | disabled | | production | cluster | GKE | RUNNING | 192 .12.1.1:3307 | enabled |","title":"Cluster List"},{"location":"rfcs/cli/0001-cli-design/#cluster-view","text":"View datafuse components in current cluster For example: In cluster profile: fusecli cluster view | NAME | CLUSTER | COMPONENT | STATUS | TLS | | query-1 | GKE | datafuse-query | running | enabled | | query-2 | GKE | datafuse-query | running | enabled | | query-3 | GKE | datafuse-query | pending | enabled | | store-1 | GKE | datafuse-store | running | enabled | | store-2 | GKE | datafuse-store | running | enabled | Check on disk utilization fusecli cluster df | NAME | COMPONENT | USED | ALWAYABLE | LOCATION | | local-disk-1 | Block | 10Gi | 90Gi | /mnt/fuse-store | | s3-disk | Object | 100 Gi | 1000Gi | s3://bucket-1/mnt/fuse-store |","title":"Cluster View"},{"location":"rfcs/cli/0001-cli-design/#cluster-delete","text":"For local profile, pids in running instances shall be killed, and for cluster profile, computing pods would be deleted, can add some flags to delete disk resources as well( RBAC needed) fusecli cluster delete","title":"Cluster delete"},{"location":"rfcs/cli/0001-cli-design/#cluster-log","text":"Show logs in current running instance show all fuse-query logs fusecli cluster log --component = query --all The command above would show all fuse-store logs fusecli cluster log --component = store --all The command above would show datafuse operator logs fusecli cluster log --component = operator","title":"Cluster log"},{"location":"rfcs/cli/0001-cli-design/#cluster-add","text":"Add component to current storage The following command add two query nodes, each need 2 cpu resource fusecli cluster add --component query --num 2 --cpu 2 --namespace = <operator namespace>","title":"Cluster add"},{"location":"rfcs/cli/0001-cli-design/#cluster-use","text":"Switch to another cluster The Command will switch to another cluster run on GKE cloud fusecli cluster use --name cloud-instance-1 --cluster GKE --kubeconfig ~/.kube/config --kubecontext gke-cloud-1","title":"Cluster use"},{"location":"rfcs/cli/0001-cli-design/#cluster-check","text":"Check whether current configuration can be deployed on given cluster It will check on port availability, and storage resource availability for deployment fusecli cluster check --profile = local It will check on cloud resources, whether compute nodes could be scheduled on given cloud platform, whether TLS configured etc fusecli cluster check -f deploy.yaml","title":"Cluster Check"},{"location":"rfcs/cli/0001-cli-design/#cluster-analyze","text":"Analyze and troubleshooting on given configuration, difference between analyze and check is that analyze is troubleshooting on a running cluster, and check mainly used for pre-fight check fusecli cluster analyze --profile = local","title":"Cluster Analyze"},{"location":"rfcs/cli/0001-cli-design/#cluster-update","text":"Update cluster to a newer version fusecli cluster update v0.5.1-nightly","title":"Cluster Update"},{"location":"rfcs/cli/0001-cli-design/#query","text":"Run query using selected client fusecli query 'SELECT * FROM TABLE1' --client = mysql","title":"Query"},{"location":"rfcs/query/0001-join-framework-design/","text":"Proposal: Join framework \u00b6 Background \u00b6 Join is one of the major features in SQL. Meanwhile, it's the most complicated part either. Thus in this section, we will make a brief introduction to types of join semantics and join algorithms. Generally, join can be categorized as following types by semantic: INNER JOIN : return all tuples satisfy the join condition LEFT OUTER JOIN : return all tuples satisfy the join condition and the rows from left table for which no row from right table satisfies the join condition RIGHT OUTER JOIN : return all tuples satisfy the join condition and the rows from right table for which no row from left table satisfies the join condition FULL OUTER JOIN : return all tuples satisfy the join condition and the rows from a table for which no row from other table satisfies the join condition CROSS JOIN : cartesian product of joined tables Besides, IN , EXISTS , NOT IN , NOT EXISTS expressions can be implemented by semi-join and anti-join (known as subquery). There are three kinds of common join algorithms: Nested-loop join Hash join Sort-merge join Nested-loop join is the basic join algorithm, it can be described as following pseudo code: // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable: for r <- innerTable: if condition(r, s) == true: insert(result, combine(r, s)) Before introducing hash join, we introduce the definition of equi join here. A equi join is join whose join condition is an equation(e.g. r.a == s.a ). For the joins whose join condition is not an equation, we call them non-equi join Hash join can only work with equi join. It can be described as two phase: build phase and probe phase . As inner table and outer table of nested-loop join, hash join will choose a table as build side and another table as probe side . The pseudo code of hash join: // R\u22c8S var build = R var probe = S var hashTable var result // Build phase for r <- build: var key = hash(r, condition) insert(hashTable, key, r) // Probe phase for s <- probe: var key = hash(s, condition) if exists(hashTable, key): var r = get(hashTable, key) insert(result, combine(r, s)) Sort-merge join will sort the joined tables if they are not sorted by join key, and then merge them like merge sort. Generally, a sort-merge join can only work with equi-join either, but it exists a band join optimization that can make sort-merge join work with some specific non-equi join. We won't talk about this here since it's a little bit out of scope. Join framework \u00b6 To implement join, we have several parts of work to be done: Support parse join statement into logical plan Support bind column reference for joined tables Support some basic heuristic optimization(e.g. outer join elimination, subquery elimination) and join reorder with choosing implementation Support some join algorithms(local execution for now but design for distributed execution) Parser & Planner \u00b6 According to ANSI-SQL specification, joins are defined in FROM clause. Besides, subquery in other clauses can be translated to join(correlated subquery will be translated to semi join or anti join) in some cases. After parsing SQL string into AST, we will build logical plan from AST with PlanParser . Following bnf definition is a simplified ANSI-SQL specification of FROM clause: < from clause > ::= FROM < table reference list > < table reference list > ::= < table reference > [ { < comma > < table reference > }... ] < table reference > ::= < table primary or joined table > < table primary or joined table > ::= < table primary > | < joined table > < table primary > ::= < table or query name > [ [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] ] | < derived table > [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] | < left paren > < joined table > < right paren > < joined table > ::= < cross join > | < qualified join > | < natural join > < cross join > ::= < table reference > CROSS JOIN < table primary > < qualified join > ::= < table reference > [ < join type > ] JOIN < table reference > < join specification > < natural join > ::= < table reference > NATURAL [ < join type > ] JOIN < table primary > < join specification > ::= < join condition > | < named columns join > < join condition > ::= ON < search condition > < named columns join > ::= USING < left paren > < join column list > < right paren > < join type > ::= INNER | < outer join type > [ OUTER ] < outer join type > ::= LEFT | RIGHT | FULL < join column list > ::= < column name list > <table reference> concated with <comma> are cross joined. And it's possible to find some conjunctions in WHERE clause as their join conditions, that is rewriting cross join into inner join. There are many queries organized in this way that doesn't explicitly specify join condition, for example TPCH query set. sqlparser library can parse a SQL string into AST. Joins are organized as a tree structure. There are following kinds of join trees: Left deep tree Right deep tree Bushy tree In left deep tree, every join node's right child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join d / \\ join c / \\ a b */ In right deep tree, every join node's left child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ a join / \\ b join / \\ c d */ In bushy tree, all children of every join node can be either result of join or table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join join / \\ / \\ a b c d */ Most of join s can be represented as left deep tree, which is easier to optimize. We can rewrite some joins to left deep tree during parsing phase. Here's an example of sqlparser AST, the comment part is simplified AST debug string: SELECT * FROM a , b NATURAL JOIN c , d ; /* Query { with: None, body: Select( Select { projection: [Wildcard], from: [ TableWithJoins { relation: Table { name: \"a\", }, joins: [] }, TableWithJoins { relation: Table { name: \"b\", }, joins: [ Join { relation: Table { name: \"c\", }, join_operator: Inner(Natural) } ] }, TableWithJoins { relation: Table { name: \"d\", }, joins: [] } ], } ), } */ The AST above can be directly represented as a bushy tree: join / \\ join d / \\ a join / \\ b c This bushy tree is equivalent to the following left deep tree so we can rewrite it in parsing phase: join / \\ join d / \\ join c / \\ a b After rewriting AST to left deep tree, we will bind the AST to concrete tables and columns with catalog. During binding, semantic checking is necessary(e.g. check whether column name is ambiguous). To implement semantic checking and simplify the binding process, we introduce Scope to represent context of each query block. It will record information about available columns in current context and which table they belong to. Columns from a parent Scope is visible to all of its children Scope . struct Scope { pub parent : Arc < Scope > , pub columns : Vec < ColumnRef > } Here's an example to explain how Scope works: CREATE TABLE t0 ( a INT ); CREATE TABLE t1 ( b INT ); CREATE TABLE t2 ( c INT ); SELECT * FROM t0 , ( SELECT b , c , c + 1 AS d FROM t1 , t2 ) t ; /* Scope root: [t0.a, t.b, t.c, t.d] | \\ | Scope t0: [a] | Scope t: [t1.b, t2.c, d] | \\ | Scope t1: [b] | Scope t2: [c] */ Since it may exist different column with same name after join, we should identify ColumnRef with a unique ColumnID . Meanwhile, correlation names are ensured to be unique, it's fine to identify them with name strings. struct ColumnRef { pub id : ColumnID , pub column_name : String , pub table_name : String } With unique ColumnID , we can check whether a query is ambiguous or not and keep their original name at the same time. For planner, we will add a variant Join for PlanNode to represent join operator: enum PlanNode { .. . Join ( JoinPlan ) } enum JoinType { Inner , LeftOuter , RightOuter , FullOuter , Cross } struct JoinPlan { pub join_type : JoinType , pub join_conditions : Vec < ExpressionPlan > , // Conjunctions of join condition pub left_child : Arc < PlanNode > , pub right_child : Arc < PlanNode > } Here's a problem that datafuse-query uses arrow::datatypes::Schema to represent data schema, while arrow::datatypes::Schema doesn't support identify columns with ColumnID natively. I suggest to introduce an internal DataSchema struct to represent data schema in datafuse-query, which can store more information and can be converted to arrow::datatypes::Schema naturally. struct DataSchema { pub columns : Vec < Arc < Column >> } struct Column { pub column_id : ColumnID , pub column_name : String , pub data_type : DataType , pub is_nullable : bool } Optimizer \u00b6 There are two kinds of optimization to be done: Heuristic optimization Cost-based optimization The heuristic optimization( RBO , aka rule-based optimization), is the optimization which can always reduce cost of a query. Since there are too many heuristic rules, we won't discuss this here. The cost-based optimization uses statistic information to calculate the cost of a query. With exploring framework(e.g. Volcano optimizer, Cascades optimizer), it can choose the best execution plan. Optimizer is the most complicated part in a SQL engine, we'd better only support limited heuristic optimization at the beginning. TODO: list common heuristic rules Execution \u00b6 As we discussed in section Background , join algorithms can be categorized into three kinds: Nested-loop join Hash join Sort-merge join Besides, there are two kinds of distributed join algorithms: Broadcast join Repartition join(aka shuffle join) We won't talk about detail of distributed join algorithms here, but we still need to consider about them. Different join algorithms have advantage on different scenarios. Nested-loop join is effective if the amount of data is relatively small. With vectorized execution model, it's natural to implement block nested-loop join, which is a refined nested-loop join algorithm. Another advantage of nested-loop join is it can work with non-equi join condition. Hash join is effective if one of the joined table is small and the other one is large. Since distributed join algorithm will always produce small tables(by partition), it fits hash join a lot. Meanwhile, vectorized hash join algorithm has been introduced by Marcin Zucowski (Co-founder of Snowflake, Phd of CWI). The disadvantage of hash join is that hash join will consume more memory than other join algorithms, and it only supports equi join. Sort-merge join is effective if inputs are sorted, while this is rarely happened. The comparison above is much biased, in fact it can hardly say that which algorithm is better. IMO, we can implement hash join and nested-loop join first since they are more common. Since we don't have infrastructure(planner, optimizer) for choosing join algorithm for now, I suggest to only implement block nested-loop join at present so we can build a complete prototype. We'are going to introduce a vectorized block nested-loop join algorithm. Pseudo code of naive nested-loop join has been introduced in Background section. As we know, nested-loop join will fetch only one row from outer table in each loop, which doen't have good locality. Block nested-loop join is a nested-loop join that will fetch a block of data in each loop. Here we introduce the naive block nested-loop join. // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable.fetchBlock(): for r <- innerTable.fetchBlock(): buffer = conditionEvalBlock(s, r) for row <- buffer: insert(result, row) In vetorized execution, we can use a bit map to indicate whether a row should be return to result set or not. Then we can materialize the result later. For example, assume we have following SQL query: CREATE TABLE t ( a int , b int ); CREATE TABLE t1 ( b int , c int ); -- insert some rows SELECT a , b , c FROM t INNER JOIN t1 ON t . b = t1 . b ; The execution plan of this query should look like: Join (t.b = t1.b) -> TableScan t -> TableScan t1 If we use the vectorized block nested-loop join algorithm introduced above, the pseudo code should look like: var leftChild: BlockStream = scan(t) var rightChild: BlockStream = scan(t1) var condition: Expression = equal(column(t.b), column(t1.b)) var result for l <- leftChild: for r <- rightChild: buffer = mergeBlock(l, r) var bitMap: Array[boolean] = condition.eval(buffer) buffer.insertColumn(bitMap) result.insertBlock(buffer) materialize(result) In datafuse-query, we can add a NestedLoopJoinTransform to implement vectorized block nested-loop join.","title":"DatafuseQuery Join"},{"location":"rfcs/query/0001-join-framework-design/#proposal-join-framework","text":"","title":"Proposal: Join framework"},{"location":"rfcs/query/0001-join-framework-design/#background","text":"Join is one of the major features in SQL. Meanwhile, it's the most complicated part either. Thus in this section, we will make a brief introduction to types of join semantics and join algorithms. Generally, join can be categorized as following types by semantic: INNER JOIN : return all tuples satisfy the join condition LEFT OUTER JOIN : return all tuples satisfy the join condition and the rows from left table for which no row from right table satisfies the join condition RIGHT OUTER JOIN : return all tuples satisfy the join condition and the rows from right table for which no row from left table satisfies the join condition FULL OUTER JOIN : return all tuples satisfy the join condition and the rows from a table for which no row from other table satisfies the join condition CROSS JOIN : cartesian product of joined tables Besides, IN , EXISTS , NOT IN , NOT EXISTS expressions can be implemented by semi-join and anti-join (known as subquery). There are three kinds of common join algorithms: Nested-loop join Hash join Sort-merge join Nested-loop join is the basic join algorithm, it can be described as following pseudo code: // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable: for r <- innerTable: if condition(r, s) == true: insert(result, combine(r, s)) Before introducing hash join, we introduce the definition of equi join here. A equi join is join whose join condition is an equation(e.g. r.a == s.a ). For the joins whose join condition is not an equation, we call them non-equi join Hash join can only work with equi join. It can be described as two phase: build phase and probe phase . As inner table and outer table of nested-loop join, hash join will choose a table as build side and another table as probe side . The pseudo code of hash join: // R\u22c8S var build = R var probe = S var hashTable var result // Build phase for r <- build: var key = hash(r, condition) insert(hashTable, key, r) // Probe phase for s <- probe: var key = hash(s, condition) if exists(hashTable, key): var r = get(hashTable, key) insert(result, combine(r, s)) Sort-merge join will sort the joined tables if they are not sorted by join key, and then merge them like merge sort. Generally, a sort-merge join can only work with equi-join either, but it exists a band join optimization that can make sort-merge join work with some specific non-equi join. We won't talk about this here since it's a little bit out of scope.","title":"Background"},{"location":"rfcs/query/0001-join-framework-design/#join-framework","text":"To implement join, we have several parts of work to be done: Support parse join statement into logical plan Support bind column reference for joined tables Support some basic heuristic optimization(e.g. outer join elimination, subquery elimination) and join reorder with choosing implementation Support some join algorithms(local execution for now but design for distributed execution)","title":"Join framework"},{"location":"rfcs/query/0001-join-framework-design/#parser-planner","text":"According to ANSI-SQL specification, joins are defined in FROM clause. Besides, subquery in other clauses can be translated to join(correlated subquery will be translated to semi join or anti join) in some cases. After parsing SQL string into AST, we will build logical plan from AST with PlanParser . Following bnf definition is a simplified ANSI-SQL specification of FROM clause: < from clause > ::= FROM < table reference list > < table reference list > ::= < table reference > [ { < comma > < table reference > }... ] < table reference > ::= < table primary or joined table > < table primary or joined table > ::= < table primary > | < joined table > < table primary > ::= < table or query name > [ [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] ] | < derived table > [ AS ] < correlation name > [ < left paren > < derived column list > < right paren > ] | < left paren > < joined table > < right paren > < joined table > ::= < cross join > | < qualified join > | < natural join > < cross join > ::= < table reference > CROSS JOIN < table primary > < qualified join > ::= < table reference > [ < join type > ] JOIN < table reference > < join specification > < natural join > ::= < table reference > NATURAL [ < join type > ] JOIN < table primary > < join specification > ::= < join condition > | < named columns join > < join condition > ::= ON < search condition > < named columns join > ::= USING < left paren > < join column list > < right paren > < join type > ::= INNER | < outer join type > [ OUTER ] < outer join type > ::= LEFT | RIGHT | FULL < join column list > ::= < column name list > <table reference> concated with <comma> are cross joined. And it's possible to find some conjunctions in WHERE clause as their join conditions, that is rewriting cross join into inner join. There are many queries organized in this way that doesn't explicitly specify join condition, for example TPCH query set. sqlparser library can parse a SQL string into AST. Joins are organized as a tree structure. There are following kinds of join trees: Left deep tree Right deep tree Bushy tree In left deep tree, every join node's right child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join d / \\ join c / \\ a b */ In right deep tree, every join node's left child is a table, for example: SELECT * FROM a , b , c , d ; /* join / \\ a join / \\ b join / \\ c d */ In bushy tree, all children of every join node can be either result of join or table, for example: SELECT * FROM a , b , c , d ; /* join / \\ join join / \\ / \\ a b c d */ Most of join s can be represented as left deep tree, which is easier to optimize. We can rewrite some joins to left deep tree during parsing phase. Here's an example of sqlparser AST, the comment part is simplified AST debug string: SELECT * FROM a , b NATURAL JOIN c , d ; /* Query { with: None, body: Select( Select { projection: [Wildcard], from: [ TableWithJoins { relation: Table { name: \"a\", }, joins: [] }, TableWithJoins { relation: Table { name: \"b\", }, joins: [ Join { relation: Table { name: \"c\", }, join_operator: Inner(Natural) } ] }, TableWithJoins { relation: Table { name: \"d\", }, joins: [] } ], } ), } */ The AST above can be directly represented as a bushy tree: join / \\ join d / \\ a join / \\ b c This bushy tree is equivalent to the following left deep tree so we can rewrite it in parsing phase: join / \\ join d / \\ join c / \\ a b After rewriting AST to left deep tree, we will bind the AST to concrete tables and columns with catalog. During binding, semantic checking is necessary(e.g. check whether column name is ambiguous). To implement semantic checking and simplify the binding process, we introduce Scope to represent context of each query block. It will record information about available columns in current context and which table they belong to. Columns from a parent Scope is visible to all of its children Scope . struct Scope { pub parent : Arc < Scope > , pub columns : Vec < ColumnRef > } Here's an example to explain how Scope works: CREATE TABLE t0 ( a INT ); CREATE TABLE t1 ( b INT ); CREATE TABLE t2 ( c INT ); SELECT * FROM t0 , ( SELECT b , c , c + 1 AS d FROM t1 , t2 ) t ; /* Scope root: [t0.a, t.b, t.c, t.d] | \\ | Scope t0: [a] | Scope t: [t1.b, t2.c, d] | \\ | Scope t1: [b] | Scope t2: [c] */ Since it may exist different column with same name after join, we should identify ColumnRef with a unique ColumnID . Meanwhile, correlation names are ensured to be unique, it's fine to identify them with name strings. struct ColumnRef { pub id : ColumnID , pub column_name : String , pub table_name : String } With unique ColumnID , we can check whether a query is ambiguous or not and keep their original name at the same time. For planner, we will add a variant Join for PlanNode to represent join operator: enum PlanNode { .. . Join ( JoinPlan ) } enum JoinType { Inner , LeftOuter , RightOuter , FullOuter , Cross } struct JoinPlan { pub join_type : JoinType , pub join_conditions : Vec < ExpressionPlan > , // Conjunctions of join condition pub left_child : Arc < PlanNode > , pub right_child : Arc < PlanNode > } Here's a problem that datafuse-query uses arrow::datatypes::Schema to represent data schema, while arrow::datatypes::Schema doesn't support identify columns with ColumnID natively. I suggest to introduce an internal DataSchema struct to represent data schema in datafuse-query, which can store more information and can be converted to arrow::datatypes::Schema naturally. struct DataSchema { pub columns : Vec < Arc < Column >> } struct Column { pub column_id : ColumnID , pub column_name : String , pub data_type : DataType , pub is_nullable : bool }","title":"Parser &amp; Planner"},{"location":"rfcs/query/0001-join-framework-design/#optimizer","text":"There are two kinds of optimization to be done: Heuristic optimization Cost-based optimization The heuristic optimization( RBO , aka rule-based optimization), is the optimization which can always reduce cost of a query. Since there are too many heuristic rules, we won't discuss this here. The cost-based optimization uses statistic information to calculate the cost of a query. With exploring framework(e.g. Volcano optimizer, Cascades optimizer), it can choose the best execution plan. Optimizer is the most complicated part in a SQL engine, we'd better only support limited heuristic optimization at the beginning. TODO: list common heuristic rules","title":"Optimizer"},{"location":"rfcs/query/0001-join-framework-design/#execution","text":"As we discussed in section Background , join algorithms can be categorized into three kinds: Nested-loop join Hash join Sort-merge join Besides, there are two kinds of distributed join algorithms: Broadcast join Repartition join(aka shuffle join) We won't talk about detail of distributed join algorithms here, but we still need to consider about them. Different join algorithms have advantage on different scenarios. Nested-loop join is effective if the amount of data is relatively small. With vectorized execution model, it's natural to implement block nested-loop join, which is a refined nested-loop join algorithm. Another advantage of nested-loop join is it can work with non-equi join condition. Hash join is effective if one of the joined table is small and the other one is large. Since distributed join algorithm will always produce small tables(by partition), it fits hash join a lot. Meanwhile, vectorized hash join algorithm has been introduced by Marcin Zucowski (Co-founder of Snowflake, Phd of CWI). The disadvantage of hash join is that hash join will consume more memory than other join algorithms, and it only supports equi join. Sort-merge join is effective if inputs are sorted, while this is rarely happened. The comparison above is much biased, in fact it can hardly say that which algorithm is better. IMO, we can implement hash join and nested-loop join first since they are more common. Since we don't have infrastructure(planner, optimizer) for choosing join algorithm for now, I suggest to only implement block nested-loop join at present so we can build a complete prototype. We'are going to introduce a vectorized block nested-loop join algorithm. Pseudo code of naive nested-loop join has been introduced in Background section. As we know, nested-loop join will fetch only one row from outer table in each loop, which doen't have good locality. Block nested-loop join is a nested-loop join that will fetch a block of data in each loop. Here we introduce the naive block nested-loop join. // R\u22c8S var innerTable = R var outerTable = S var result for s <- outerTable.fetchBlock(): for r <- innerTable.fetchBlock(): buffer = conditionEvalBlock(s, r) for row <- buffer: insert(result, row) In vetorized execution, we can use a bit map to indicate whether a row should be return to result set or not. Then we can materialize the result later. For example, assume we have following SQL query: CREATE TABLE t ( a int , b int ); CREATE TABLE t1 ( b int , c int ); -- insert some rows SELECT a , b , c FROM t INNER JOIN t1 ON t . b = t1 . b ; The execution plan of this query should look like: Join (t.b = t1.b) -> TableScan t -> TableScan t1 If we use the vectorized block nested-loop join algorithm introduced above, the pseudo code should look like: var leftChild: BlockStream = scan(t) var rightChild: BlockStream = scan(t1) var condition: Expression = equal(column(t.b), column(t1.b)) var result for l <- leftChild: for r <- rightChild: buffer = mergeBlock(l, r) var bitMap: Array[boolean] = condition.eval(buffer) buffer.insertColumn(bitMap) result.insertBlock(buffer) materialize(result) In datafuse-query, we can add a NestedLoopJoinTransform to implement vectorized block nested-loop join.","title":"Execution"},{"location":"rfcs/query/0002-plan-expression/","text":"Expression and plan builder \u00b6 Summary \u00b6 Logic plan and expression play a big role throughout the life cycle of SQL query. This doc is intended to explain the new design of expressions and plan builder. Expression \u00b6 Alias Expression \u00b6 Aliasing is useful in SQL, we can alias a complex expression as a short alias name. Such as: select a + 3 as b . In the standard SQL protocol, aliasing can work in: Group By, eg: select a + 3 as b, count(1) from table group by b Having, eg: select a + 3 as b, count(1) as c from table group by b having c > 0 Order By: eg: select a + 3 as b from table order by b Notes ClickHouse has extended the usage of expression alias, it can be work in: recursive alias expression: eg: select a + 1 as b, b + 1 as c filter: eg: select a + 1 as b, b + 1 as c from table where c > 0 Note Currently we do not support clickhouse style alias expression. It can be implemented later. For expression alias, we only handle it at last, in projection stage. But We have to replace the alias of the expression as early as possible to prevent ambiguity later. Eg: select number + 1 as c, sum(number) from numbers(10) group by c having c > 3 order by c limit 10 Firstly, we can scan all the alias expressions from projection ASTs. c ---> (number + 1) Then we replaced the alias into the corresponding expression in having , order by , group by clause. So the query will be: select number + 1 as c, sum(number) from numbers(10) group by (number + 1) having (number + 1) > 3 order by (number + 1) limit 10 At last, when the query is finished, we apply the projection to rename the column (number+1) to c Let's take a look at the explain result of this query: | Limit: 10 Projection: (number + 1) as c:UInt64, sum(number):UInt64 Sort: (number + 1):UInt64 Having: ((number + 1) > 3) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum(number)]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum(number)]] Expression: (number + 1):UInt64, number:UInt64 (Before GroupBy) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] We can see we do not need to care about aliasing until the projection, so it will be very convenient to apply other expressions. Materialized Expression \u00b6 Materialized expression processing is that we can rebase the expression as a ExpressionColumn if the same expression is already processed upstream. Eg: select number + 1 as c, sum(number) as d group by c having number + 1 > 3 order by d desc After aliases replacement, we will know that order by is sum(number) , but sum(number) is already processed during the aggregating stage, so we can rebase the order by expression SortExpression { ... } to Column(\"sum(number)\") , this could remove useless calculation of same expressions. So number + 1 in having can also apply to rebase the expression. Expression Functions \u00b6 There are many kinds of expression functions. ScalarFunctions, One-to-one calculation process, the result rows is same as the input rows. eg: select database() AggregateFunctions, Many-to-one calculation process, eg: select sum(number) BinaryFunctions, a special kind of \u00b7ScalarFunctions\u00b7 eg: select 1 + 2 ... For ScalarFunctions, we really don't care about the whole block, we just care about the columns involved by the arguments. sum(number) just care about the Column which named number . And the result is also a column, so we have the virtual method in IFunction is: fn eval ( & self , columns : & [ DataColumn ], _input_rows : usize ) -> Result < DataColumn > ; For AggregateFunctions, we should keep the state in the corresponding function instance to apply the two-level merge, we have the following virtual method in IAggregateFunction : fn accumulate ( & mut self , columns : & [ DataColumn ], _input_rows : usize ) -> Result < () > ; fn accumulate_result ( & self ) -> Result < Vec < DataValue >> ; fn merge ( & mut self , _states : & [ DataValue ]) -> Result < () > ; fn merge_result ( & self ) -> Result < DataValue > ; The process is accumulate (apply data to the function) \u2192 accumulate_result (to get the current state) \u2192 merge (merge current state from other state) ---> merge_result (to get the final result value) ps: We don't store the arguments types and arguments names in functions, we can store them later if we need. Column \u00b6 Block is the unit of data passed between streams for pipeline processing, while Column is the unit of data passed between expressions. So in the view of expression(functions, literal, ...), everything is Column , we have DataColumn to represent a column. #[derive(Clone, Debug)] pub enum DataColumn { // Array of values. Array ( DataArrayRef ), // A Single value. Constant ( DataValue , usize ) } DataColumn::Constant is like ConstantColumn in ClickHouse . Note: We don't have ScalarValue , because it can be known as Constant(DataValue, 1) , and there is DataValue struct. Expression chain and expression executor \u00b6 Currently, we can collect the inner expression from expressions to build ExpressionChain. This could be done by Depth-first-search visiting. ExpressionFunction: number + (number + 1) will be : [ ExpressionColumn(number), ExpressionColumn(number), ExpressionLiteral(1), ExpressionBinary('+', 'number', '1'), ExpressionBinary('+', 'number', '(number + 1)') ] . We have the ExpressionExecutor the execute the expression chain, during the execution, we don't need to care about the kind of the arguments. We just consider them as ColumnExpression from upstream, so we just fetch the column number and the column (number + 1) from the block. Plan Builder \u00b6 None aggregation query \u00b6 This is for queries without group by and aggregate functions . Eg: explain select number + 1 as b from numbers(10) where number + 1 > 3 order by number + 3 | explain | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: (number + 1) as b:UInt64 Sort: (number + 3):UInt64 Expression: (number + 1):UInt64, (number + 3):UInt64 (Before OrderBy) Filter: ((number + 1) > 3) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 1) > 3 , the schema keeps the same, schema \u2192 [number] Expression: we will collect expressions from order by and having clauses to apply the expression, schema \u2192 [number, number + 1, number + 3] Sort: since we already have the number + 1 in the input plan, so the sorting will consider number + 1 as ColumnExpression , schema \u2192 [number, number + 1, number + 3] Projection: applying the aliases and projection the columns, schema \u2192 [b] Aggregation query \u00b6 To build Aggregation query, there will be more complex than the previous one. Eg: explain select number + 1 as b, sum(number + 2 ) + 4 as c from numbers(10) where number + 3 > 0 group by number + 1 having c > 3 and sum(number + 4) + 1 > 4 order by sum(number + 5) + 1; | Projection: (number + 1) as b:UInt64, (sum((number + 2)) + 4) as c:UInt64 Sort: sum((number + 5)):UInt64 Having: (((sum((number + 2)) + 4) > 3) AND (sum((number + 4)) > 0)) Expression: (number + 1):UInt64, (sum((number + 2)) + 4):UInt64, sum((number + 5)):UInt64 (Before OrderBy) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] Expression: (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Filter: ((number + 3) > 0) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 3) > 0 , the schema keeps the same, schema \u2192 [number] Expression: Before group by (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Before GroupBy, We must visit all the expression in projections , having , group by to collect the expressions and aggregate functions, schema \u2192 [number, number + 1, number + 2, number + 4, number + 5] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] , note that: the expressions are already materialized in upstream, so we just conside all the arguments as columns. AggregatorFinal, schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4))] Expression: schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4)), sum((number + 2)) + 4, sum((number + 5)) + 1] Sort: the schema keeps the same Projection: schema \u2192 b, c","title":"DatafuseQuery Expression"},{"location":"rfcs/query/0002-plan-expression/#expression-and-plan-builder","text":"","title":"Expression and plan builder"},{"location":"rfcs/query/0002-plan-expression/#summary","text":"Logic plan and expression play a big role throughout the life cycle of SQL query. This doc is intended to explain the new design of expressions and plan builder.","title":"Summary"},{"location":"rfcs/query/0002-plan-expression/#expression","text":"","title":"Expression"},{"location":"rfcs/query/0002-plan-expression/#alias-expression","text":"Aliasing is useful in SQL, we can alias a complex expression as a short alias name. Such as: select a + 3 as b . In the standard SQL protocol, aliasing can work in: Group By, eg: select a + 3 as b, count(1) from table group by b Having, eg: select a + 3 as b, count(1) as c from table group by b having c > 0 Order By: eg: select a + 3 as b from table order by b Notes ClickHouse has extended the usage of expression alias, it can be work in: recursive alias expression: eg: select a + 1 as b, b + 1 as c filter: eg: select a + 1 as b, b + 1 as c from table where c > 0 Note Currently we do not support clickhouse style alias expression. It can be implemented later. For expression alias, we only handle it at last, in projection stage. But We have to replace the alias of the expression as early as possible to prevent ambiguity later. Eg: select number + 1 as c, sum(number) from numbers(10) group by c having c > 3 order by c limit 10 Firstly, we can scan all the alias expressions from projection ASTs. c ---> (number + 1) Then we replaced the alias into the corresponding expression in having , order by , group by clause. So the query will be: select number + 1 as c, sum(number) from numbers(10) group by (number + 1) having (number + 1) > 3 order by (number + 1) limit 10 At last, when the query is finished, we apply the projection to rename the column (number+1) to c Let's take a look at the explain result of this query: | Limit: 10 Projection: (number + 1) as c:UInt64, sum(number):UInt64 Sort: (number + 1):UInt64 Having: ((number + 1) > 3) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum(number)]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum(number)]] Expression: (number + 1):UInt64, number:UInt64 (Before GroupBy) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] We can see we do not need to care about aliasing until the projection, so it will be very convenient to apply other expressions.","title":"Alias Expression"},{"location":"rfcs/query/0002-plan-expression/#materialized-expression","text":"Materialized expression processing is that we can rebase the expression as a ExpressionColumn if the same expression is already processed upstream. Eg: select number + 1 as c, sum(number) as d group by c having number + 1 > 3 order by d desc After aliases replacement, we will know that order by is sum(number) , but sum(number) is already processed during the aggregating stage, so we can rebase the order by expression SortExpression { ... } to Column(\"sum(number)\") , this could remove useless calculation of same expressions. So number + 1 in having can also apply to rebase the expression.","title":"Materialized Expression"},{"location":"rfcs/query/0002-plan-expression/#expression-functions","text":"There are many kinds of expression functions. ScalarFunctions, One-to-one calculation process, the result rows is same as the input rows. eg: select database() AggregateFunctions, Many-to-one calculation process, eg: select sum(number) BinaryFunctions, a special kind of \u00b7ScalarFunctions\u00b7 eg: select 1 + 2 ... For ScalarFunctions, we really don't care about the whole block, we just care about the columns involved by the arguments. sum(number) just care about the Column which named number . And the result is also a column, so we have the virtual method in IFunction is: fn eval ( & self , columns : & [ DataColumn ], _input_rows : usize ) -> Result < DataColumn > ; For AggregateFunctions, we should keep the state in the corresponding function instance to apply the two-level merge, we have the following virtual method in IAggregateFunction : fn accumulate ( & mut self , columns : & [ DataColumn ], _input_rows : usize ) -> Result < () > ; fn accumulate_result ( & self ) -> Result < Vec < DataValue >> ; fn merge ( & mut self , _states : & [ DataValue ]) -> Result < () > ; fn merge_result ( & self ) -> Result < DataValue > ; The process is accumulate (apply data to the function) \u2192 accumulate_result (to get the current state) \u2192 merge (merge current state from other state) ---> merge_result (to get the final result value) ps: We don't store the arguments types and arguments names in functions, we can store them later if we need.","title":"Expression Functions"},{"location":"rfcs/query/0002-plan-expression/#column","text":"Block is the unit of data passed between streams for pipeline processing, while Column is the unit of data passed between expressions. So in the view of expression(functions, literal, ...), everything is Column , we have DataColumn to represent a column. #[derive(Clone, Debug)] pub enum DataColumn { // Array of values. Array ( DataArrayRef ), // A Single value. Constant ( DataValue , usize ) } DataColumn::Constant is like ConstantColumn in ClickHouse . Note: We don't have ScalarValue , because it can be known as Constant(DataValue, 1) , and there is DataValue struct.","title":"Column"},{"location":"rfcs/query/0002-plan-expression/#expression-chain-and-expression-executor","text":"Currently, we can collect the inner expression from expressions to build ExpressionChain. This could be done by Depth-first-search visiting. ExpressionFunction: number + (number + 1) will be : [ ExpressionColumn(number), ExpressionColumn(number), ExpressionLiteral(1), ExpressionBinary('+', 'number', '1'), ExpressionBinary('+', 'number', '(number + 1)') ] . We have the ExpressionExecutor the execute the expression chain, during the execution, we don't need to care about the kind of the arguments. We just consider them as ColumnExpression from upstream, so we just fetch the column number and the column (number + 1) from the block.","title":"Expression chain and expression executor"},{"location":"rfcs/query/0002-plan-expression/#plan-builder","text":"","title":"Plan Builder"},{"location":"rfcs/query/0002-plan-expression/#none-aggregation-query","text":"This is for queries without group by and aggregate functions . Eg: explain select number + 1 as b from numbers(10) where number + 1 > 3 order by number + 3 | explain | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: (number + 1) as b:UInt64 Sort: (number + 3):UInt64 Expression: (number + 1):UInt64, (number + 3):UInt64 (Before OrderBy) Filter: ((number + 1) > 3) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] | +-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ 1 row in set (0.02 sec) The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 1) > 3 , the schema keeps the same, schema \u2192 [number] Expression: we will collect expressions from order by and having clauses to apply the expression, schema \u2192 [number, number + 1, number + 3] Sort: since we already have the number + 1 in the input plan, so the sorting will consider number + 1 as ColumnExpression , schema \u2192 [number, number + 1, number + 3] Projection: applying the aliases and projection the columns, schema \u2192 [b]","title":"None aggregation query"},{"location":"rfcs/query/0002-plan-expression/#aggregation-query","text":"To build Aggregation query, there will be more complex than the previous one. Eg: explain select number + 1 as b, sum(number + 2 ) + 4 as c from numbers(10) where number + 3 > 0 group by number + 1 having c > 3 and sum(number + 4) + 1 > 4 order by sum(number + 5) + 1; | Projection: (number + 1) as b:UInt64, (sum((number + 2)) + 4) as c:UInt64 Sort: sum((number + 5)):UInt64 Having: (((sum((number + 2)) + 4) > 3) AND (sum((number + 4)) > 0)) Expression: (number + 1):UInt64, (sum((number + 2)) + 4):UInt64, sum((number + 5)):UInt64 (Before OrderBy) AggregatorFinal: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] RedistributeStage[state: AggregatorMerge, id: 0] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] Expression: (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Filter: ((number + 3) > 0) ReadDataSource: scan partitions: [4], scan schema: [number:UInt64], statistics: [read_rows: 10, read_bytes: 80] The build process is SourcePlan : schema \u2192 [number] FilterPlan: filter expression is (number + 3) > 0 , the schema keeps the same, schema \u2192 [number] Expression: Before group by (number + 1):UInt64, (number + 2):UInt64, (number + 5):UInt64, (number + 4):UInt64 (Before GroupBy) Before GroupBy, We must visit all the expression in projections , having , group by to collect the expressions and aggregate functions, schema \u2192 [number, number + 1, number + 2, number + 4, number + 5] AggregatorPartial: groupBy=[[(number + 1)]], aggr=[[sum((number + 2)), sum((number + 5)), sum((number + 4))]] , note that: the expressions are already materialized in upstream, so we just conside all the arguments as columns. AggregatorFinal, schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4))] Expression: schema \u2192 [number + 1, sum((number + 2)), sum((number + 5)), sum((number + 4)), sum((number + 2)) + 4, sum((number + 5)) + 1] Sort: the schema keeps the same Projection: schema \u2192 b, c","title":"Aggregation query"},{"location":"rfcs/query/0003-data-shuffle/","text":"Distributed query and data shuffle \u00b6 Summary \u00b6 Distributed query is distributed database necessary feature. This doc is intended to explain the distributed query and its data flow design. Local query \u00b6 Let's see how normal queries run on a single database node. ' +------+ +------------+ +---------+ ' | | AST | | Plan | | ' SQL--->|Parser+------>|Plan Builder+----->|Optimizer| ' | | | | | | ' +------+ +------------+ +---+-----+ ' | Plan ' v ' +----------+ +-----------+ ' | | Processor | | ' Data <------+DataStream|<-----------+Interpreter| ' | | | | ' +----------+ +-----------+ Parser and AST \u00b6 DataFuse uses the third-party SQL parser and its AST. For more information, see: https://github.com/ballista-compute/sqlparser-rs PlanBuilder and Plan \u00b6 A query plan (or query execution plan) is a sequence of steps used to access data in DataFuse. It is built by PlanBuilder from AST. We also use tree to describe it(similar to AST). But it has some differences with AST: Plan is serializable and deserializable. Plan is grammatically safe, we don't worry about it. Plan is used to describe the computation and data dependency, not related to syntax priority We can show it with EXPLAIN SELECT ... mysql> EXPLAIN SELECT number % 3 AS key, SUM(number) AS value FROM numbers(1000) WHERE number > 10 AND number < 990 GROUP BY key ORDER BY key ASC LIMIT 10; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Limit: 10 Projection: (number % 3) as key:UInt8, SUM(number) as value:UInt64 Sort: (number % 3):UInt8, AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] Expression: (number % 3):UInt8, number:UInt64 (Before GroupBy) Filter: ((number > 10) AND (number < 990)) ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000, read_bytes: 8000] | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Optimizer and Plan \u00b6 For a query, especially a complex query, you can used different plan combinations, orders and structures to get the data . Each of the different ways will get different processing time. So we need to find a reasonable plan combination way in the shortest time, which is what the optimizer does. Interpreter and Processor \u00b6 The interpreter constructs the optimized plan into an executable data stream. We pull the result of SQL by pulling the data in the stream. The calculation logic of each operator in SQL corresponds to a processor, such as FilterPlan -> FilterProcessor, ProjectionPlan -> ProjectionProcessor Distributed query \u00b6 In the cluster mode, we may have to process with some problems different from the standalone mode. In distributed mode, the tables to be queried are always distributed in different nodes For some scenarios, distributed processing is always efficient, such as GROUP BY with keys, JOIN For some scenarios, we have no way of distributed processing, such as LIMIT, GROUP BY without keys In order to ensure fast calculation, we need to coordinate the location of calculation and data. Let's see how normal queries run on a database cluster. ' +------+ +------------+ +------------------+ ' | | AST | | Plan | Optimizer | ' SQL--->|Parser+------>|Plan Builder+----->| | ' | | | | | ScatterOptimizer | ' +------+ +------------+ +--------+---------+ ' | ' +--------------+ | ' | | | ' +--+ FlightStream | <------+ | Plan ' | | | | | ' | +--------------+ | | ' | | | ' | | | ' | | Flight RPC v ' +----------+ Processor | +--------------+ | +----------------+ ' | | RemoteProcessor | | | | do_action | Interpreter | ' Data<--+DataStream|<----------------+--+ FlightStream | <------+------------------+ | ' | | | | | | | PlanRescheduler| ' +----------+ | +--------------+ | +----------------+ ' | | ' | | ' | | ' | | ' | +--------------+ | ' | | | | ' +--+ FlightStream | <------+ ' | | ' +--------------+ ScatterOptimizer and StagePlan \u00b6 In Datafuse, we use ScatterOptimizer to decide the distributed computing of query. In other words, distributed query is an optimization of standalone query. In ScatterOptimizer, we traverse all the plans of the query and rewrite the plan of interest(rewrite as StagePlan { kind:StageKind, input:Self }), where input is the rewritten plan, and kind is an enumeration(Normal: data is shuffled again, Expansive: data spreads from one node to multiple nodes, Convergent: data aggregation from multiple nodes to one node) PlanScheduler and RemoteProcessor \u00b6 In cluster mode, we extract all the StagePlans in the plan optimized by ScatterOptimizer and send them to the corresponding nodes in the cluster according to the kind. For example: mysql> EXPLAIN SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_local(1000000000) GROUP BY user); +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: argMin(user, salary):UInt64 <-- execute in local node AggregatorFinal: groupBy=[[]], aggr=[[argMin(user, salary)]] RedistributeStage[expr: 0] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[]], aggr=[[argMin(user, salary)]] Projection: sum(number) as salary:UInt64, (number % 3) as user:UInt8 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum(number)]] RedistributeStage[expr: sipHash(_group_by_key)] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum(number)]] Expression: (number % 3):UInt8, number:UInt64 (Before GroupBy) RedistributeStage[expr: blockNumber()] <-- execute in local node ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000000000, read_bytes: 8000000000] | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ Flight API DataStream \u00b6 We need to fetch the results of the plans sent to other nodes for execution in some way. FuseData uses the third-party library arrow-flight. more information:[ https://github.com/apache/arrow-rs/tree/master/arrow-flight ]","title":"DatafuseQuery Shuffle"},{"location":"rfcs/query/0003-data-shuffle/#distributed-query-and-data-shuffle","text":"","title":"Distributed query and data shuffle"},{"location":"rfcs/query/0003-data-shuffle/#summary","text":"Distributed query is distributed database necessary feature. This doc is intended to explain the distributed query and its data flow design.","title":"Summary"},{"location":"rfcs/query/0003-data-shuffle/#local-query","text":"Let's see how normal queries run on a single database node. ' +------+ +------------+ +---------+ ' | | AST | | Plan | | ' SQL--->|Parser+------>|Plan Builder+----->|Optimizer| ' | | | | | | ' +------+ +------------+ +---+-----+ ' | Plan ' v ' +----------+ +-----------+ ' | | Processor | | ' Data <------+DataStream|<-----------+Interpreter| ' | | | | ' +----------+ +-----------+","title":"Local query"},{"location":"rfcs/query/0003-data-shuffle/#parser-and-ast","text":"DataFuse uses the third-party SQL parser and its AST. For more information, see: https://github.com/ballista-compute/sqlparser-rs","title":"Parser and AST"},{"location":"rfcs/query/0003-data-shuffle/#planbuilder-and-plan","text":"A query plan (or query execution plan) is a sequence of steps used to access data in DataFuse. It is built by PlanBuilder from AST. We also use tree to describe it(similar to AST). But it has some differences with AST: Plan is serializable and deserializable. Plan is grammatically safe, we don't worry about it. Plan is used to describe the computation and data dependency, not related to syntax priority We can show it with EXPLAIN SELECT ... mysql> EXPLAIN SELECT number % 3 AS key, SUM(number) AS value FROM numbers(1000) WHERE number > 10 AND number < 990 GROUP BY key ORDER BY key ASC LIMIT 10; +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Limit: 10 Projection: (number % 3) as key:UInt8, SUM(number) as value:UInt64 Sort: (number % 3):UInt8, AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[SUM(number)]] Expression: (number % 3):UInt8, number:UInt64 (Before GroupBy) Filter: ((number > 10) AND (number < 990)) ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000, read_bytes: 8000] | +----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"PlanBuilder and Plan"},{"location":"rfcs/query/0003-data-shuffle/#optimizer-and-plan","text":"For a query, especially a complex query, you can used different plan combinations, orders and structures to get the data . Each of the different ways will get different processing time. So we need to find a reasonable plan combination way in the shortest time, which is what the optimizer does.","title":"Optimizer and Plan"},{"location":"rfcs/query/0003-data-shuffle/#interpreter-and-processor","text":"The interpreter constructs the optimized plan into an executable data stream. We pull the result of SQL by pulling the data in the stream. The calculation logic of each operator in SQL corresponds to a processor, such as FilterPlan -> FilterProcessor, ProjectionPlan -> ProjectionProcessor","title":"Interpreter and Processor"},{"location":"rfcs/query/0003-data-shuffle/#distributed-query","text":"In the cluster mode, we may have to process with some problems different from the standalone mode. In distributed mode, the tables to be queried are always distributed in different nodes For some scenarios, distributed processing is always efficient, such as GROUP BY with keys, JOIN For some scenarios, we have no way of distributed processing, such as LIMIT, GROUP BY without keys In order to ensure fast calculation, we need to coordinate the location of calculation and data. Let's see how normal queries run on a database cluster. ' +------+ +------------+ +------------------+ ' | | AST | | Plan | Optimizer | ' SQL--->|Parser+------>|Plan Builder+----->| | ' | | | | | ScatterOptimizer | ' +------+ +------------+ +--------+---------+ ' | ' +--------------+ | ' | | | ' +--+ FlightStream | <------+ | Plan ' | | | | | ' | +--------------+ | | ' | | | ' | | | ' | | Flight RPC v ' +----------+ Processor | +--------------+ | +----------------+ ' | | RemoteProcessor | | | | do_action | Interpreter | ' Data<--+DataStream|<----------------+--+ FlightStream | <------+------------------+ | ' | | | | | | | PlanRescheduler| ' +----------+ | +--------------+ | +----------------+ ' | | ' | | ' | | ' | | ' | +--------------+ | ' | | | | ' +--+ FlightStream | <------+ ' | | ' +--------------+","title":"Distributed query"},{"location":"rfcs/query/0003-data-shuffle/#scatteroptimizer-and-stageplan","text":"In Datafuse, we use ScatterOptimizer to decide the distributed computing of query. In other words, distributed query is an optimization of standalone query. In ScatterOptimizer, we traverse all the plans of the query and rewrite the plan of interest(rewrite as StagePlan { kind:StageKind, input:Self }), where input is the rewritten plan, and kind is an enumeration(Normal: data is shuffled again, Expansive: data spreads from one node to multiple nodes, Convergent: data aggregation from multiple nodes to one node)","title":"ScatterOptimizer and StagePlan"},{"location":"rfcs/query/0003-data-shuffle/#planscheduler-and-remoteprocessor","text":"In cluster mode, we extract all the StagePlans in the plan optimized by ScatterOptimizer and send them to the corresponding nodes in the cluster according to the kind. For example: mysql> EXPLAIN SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_local(1000000000) GROUP BY user); +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | explain | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+ | Projection: argMin(user, salary):UInt64 <-- execute in local node AggregatorFinal: groupBy=[[]], aggr=[[argMin(user, salary)]] RedistributeStage[expr: 0] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[]], aggr=[[argMin(user, salary)]] Projection: sum(number) as salary:UInt64, (number % 3) as user:UInt8 AggregatorFinal: groupBy=[[(number % 3)]], aggr=[[sum(number)]] RedistributeStage[expr: sipHash(_group_by_key)] <-- execute in all nodes of the cluster AggregatorPartial: groupBy=[[(number % 3)]], aggr=[[sum(number)]] Expression: (number % 3):UInt8, number:UInt64 (Before GroupBy) RedistributeStage[expr: blockNumber()] <-- execute in local node ReadDataSource: scan partitions: [8], scan schema: [number:UInt64], statistics: [read_rows: 1000000000, read_bytes: 8000000000] | +-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","title":"PlanScheduler and RemoteProcessor"},{"location":"rfcs/query/0003-data-shuffle/#flight-api-datastream","text":"We need to fetch the results of the plans sent to other nodes for execution in some way. FuseData uses the third-party library arrow-flight. more information:[ https://github.com/apache/arrow-rs/tree/master/arrow-flight ]","title":"Flight API DataStream"},{"location":"rfcs/query/0004-performance-test/","text":"Design of test infra for performance test \u00b6 Currently, we have already supports to run performance tests locally in tests/perfs directory, and here we need to support performance testing in CI Decoupling compute and storage allow datafuse to integrate into kubernetes easily. With the support of kubernetes platform, datafuse benchmarking could achieve the following advantages: 1. Stable benchmarking results, with containerization and cgroup, easy testing instance could have idempotence computing resource 2. Elastic, can expand or shrink tests on demand and also supports to run test locally(minikube + DinD) or on the cloud 3. For following tests, we need to test on TPC benchmark and integrate datafuse-store storage layer to test infrastructure, thus far kubernetes can help instance scaling easily Goals \u00b6 Fast CI speed is desired, By design one performance testing should not exceed two hours(including docker build time and performance testing running time) Expandable: supports to deploy performance tests on scale, and also supports to deploy on a single machine for affordable CI Cloud Native environment supports: Should be able to deploy whole platform on different cloud providers like GKE, EKS High Availability: both webhook and runner should support to self-healing and do not have single point failure Observability: whole process should be observable, should collect logs for performance running instances and collect compare report results Non Goals \u00b6 Hybrid Cloud not supported for alpha version Networking optimization for github action part(typically CI fail is caused by networking problem Dashboard(Currently, prototype implemented, but priority here is low) Performance Test API \u00b6 Support three semantics /run-perf <branch-name> Compare performance difference between current pull requests\u2019 latest SHA build and given branch name Branch-name supports: 1. Master branch: master (some repo is main) 2. Release tag branch: i,e v1.1.1-nightly 3. Latest tag: fetch the latest tag in the github repo /rerun-perf <branch-name> Similar to run-perf part, the ONLY difference is that it would bypass docker build part and assume performance docker build images are ready for test Examples: /run-perf master It will compare performance between current PR\u2019s latest commit and master branch /run-perf v1.1.1-nightly It will compare performance between current PR\u2019s latest commit and release tag v1.1.1-nightly /run-perf latest It will compare performance between current PR\u2019s latest commit and the latest release tag /rerun-perf master Do the same thing as /run-perf master did, but will skip docker image building steps For more information please checkout test-infra","title":"Performance Test"},{"location":"rfcs/query/0004-performance-test/#design-of-test-infra-for-performance-test","text":"Currently, we have already supports to run performance tests locally in tests/perfs directory, and here we need to support performance testing in CI Decoupling compute and storage allow datafuse to integrate into kubernetes easily. With the support of kubernetes platform, datafuse benchmarking could achieve the following advantages: 1. Stable benchmarking results, with containerization and cgroup, easy testing instance could have idempotence computing resource 2. Elastic, can expand or shrink tests on demand and also supports to run test locally(minikube + DinD) or on the cloud 3. For following tests, we need to test on TPC benchmark and integrate datafuse-store storage layer to test infrastructure, thus far kubernetes can help instance scaling easily","title":"Design of test infra for performance test"},{"location":"rfcs/query/0004-performance-test/#goals","text":"Fast CI speed is desired, By design one performance testing should not exceed two hours(including docker build time and performance testing running time) Expandable: supports to deploy performance tests on scale, and also supports to deploy on a single machine for affordable CI Cloud Native environment supports: Should be able to deploy whole platform on different cloud providers like GKE, EKS High Availability: both webhook and runner should support to self-healing and do not have single point failure Observability: whole process should be observable, should collect logs for performance running instances and collect compare report results","title":"Goals"},{"location":"rfcs/query/0004-performance-test/#non-goals","text":"Hybrid Cloud not supported for alpha version Networking optimization for github action part(typically CI fail is caused by networking problem Dashboard(Currently, prototype implemented, but priority here is low)","title":"Non Goals"},{"location":"rfcs/query/0004-performance-test/#performance-test-api","text":"Support three semantics /run-perf <branch-name> Compare performance difference between current pull requests\u2019 latest SHA build and given branch name Branch-name supports: 1. Master branch: master (some repo is main) 2. Release tag branch: i,e v1.1.1-nightly 3. Latest tag: fetch the latest tag in the github repo /rerun-perf <branch-name> Similar to run-perf part, the ONLY difference is that it would bypass docker build part and assume performance docker build images are ready for test Examples: /run-perf master It will compare performance between current PR\u2019s latest commit and master branch /run-perf v1.1.1-nightly It will compare performance between current PR\u2019s latest commit and release tag v1.1.1-nightly /run-perf latest It will compare performance between current PR\u2019s latest commit and the latest release tag /rerun-perf master Do the same thing as /run-perf master did, but will skip docker image building steps For more information please checkout test-infra","title":"Performance Test API"},{"location":"rfcs/store/0001-store-design/","text":"DatafuseStore is the storage layer in charge of: - meta data storage such as user, db, table and schema. - blocks life cycle management such as allocation, compaction etc. - data/metadata consistency and reliability. DatafuseQuery(client) | | rpc v DatafuseStore | flightServer // network, auth | | | v | Handler // execution engine | | | v | IFileSystem // abstract storage layer IFileSystem \u00b6 IFileSystem defines an abstract storage layer that DatafuseStore would runs on. An IFileSystem impl in the cluster is the only stateful component. Local FS: impl IFileSystem API and use a local disk folder as storage. Suitable for a single node DatafuseQuery deployment. DFS: impl IFileSystem and setup an aws-S3 like storage service. A DFS organizes multiple LocalFS with a centralized meta data service. Object Storage Adapters: an IFileSystem impl that builds upon an object storage service on cloud. IFileSystem defines following API-s: add : AKA put-if-absent: add a file only if it is absent. read_all : read all bytes of a file. list : retrieve a list of files with specified prefix. TODO API \u00b6 DatafuseQuery and DatafuseStore talks arrow-flight protocol. Schema related operations such as create table or create database are wrapped with a FlightService::do_action RPC. Data operation such as reading or writing a block are done with FlightService::do_get and FlightService::do_put . See common/flights/src/store_client.rs . DFS \u00b6 The most important component in DatafuseStore is the DFS. DFS mainly consists of two parts: the meta data cluster and block storage cluster. Block cluster is unaware of data placement and is purely a set of servers providing object like write and read API. Data placement is part of the meta data and is stored in the meta cluster. Meta data cluster \u00b6 All meta(the cluster meta and file keys) has a copy that resides in memory on every node for instant access. Every update to meta is done by committing a raft log of the updated entry into meta cluster. A cluster keeps its meta data in a raft group with typically 5 candidate nodes and all other nodes as learners. Candidate nodes provides meta data read and write API. Every other node is a learner , which does not elect but just subscribes meta change message from the 5 candidates. In-process metadata components \u00b6 A DatafuseStore process includes two grpc API: the flight service and the meta service. Meta related components are wrapped into MetaNode , in which a Raft instance is maintained along with storage and network engines. MetaNode is the only entry for other DatafuseStore components to access meta data. RaftNode communicates with remote RaftNode through Network , which send messages to meta-grpc service on other DatafuseStore nodes. Network relies on Storage to find out info of other nodes. DatafuseStore components: .---------------------------. | | | flight-grpc meta-grpc | | | | | | '--. .-----' | | v v | | MetaNode | | | | | | | v | | | RaftNode | | | .--' | | | v v v | | Storage <- Network | | | '---------------------------' Meta data structure \u00b6 Meta data includes hardware information: nodes, the file information: keys and data placement information: slots. message Meta { map < string , string > keys ; repeated Slot slots ; map < int64 , Node > nodes } message Node { int64 NodeId repeated string Addresses } message Slot { repeated int64 node_ids ; } File format \u00b6 A data block in DFS or local-FS is a complete Parquet data, with schema embedded. The schema in block should always be identical to the schema stored in table-meta. Otherwise it is a severe bug. Parquet has its own segmentation and index in it which is similar to ClickHouse file structure. See: https://parquet.apache.org/documentation/latest/ A schema file such as table or database is in protobuf format. Data placement \u00b6 A file in DFS has 3 copies. A file is identified with a unique key . Every key is assigned to a virtual allocation unit slot by some hash algo. A slot is assigned to 3 nodes. The slot-to-nodes mapping is part of the DFS meta data. Replication \u00b6 Once a data block is persisted on local fs and the corresponding meta data is committed, DFS ack the client an OK message. Every node is either a follower or leaner of the meta data raft group thus will receive the meta changes. If a node found that a key is uploaded and the slot for the key is on this node, it pulls the data block from the uploading node. Meta cluster startup \u00b6 A DatafuseStore cluster is stateful thus the startup is done in several steps: Boot up the first node in a cluster, by calling MetaNode::boot() . This func creates an empty raft instance add initialize itself as the leader of the solo cluster. Creating node is done with MetaNode::boot_non_voter() . This func does nothing more than initializing a raft instance. It returns the MetaNode instance with network API ready for accepting raft communications. At this point it is unaware of anything about the cluster. To add this new node to a cluster, send an Cmd::AddNode request to the leader. The leader will then commit the node info into its own storage and start sending raft snapshot and logs to the new node. When a node is shutting down and restarted, just the MetaNode::new() is used to start the meta-service. When a node is restarted: A candidate(AKA voter) that becomes the new leader is able to find out every node from its local storage and then add them as non-voter in order to replicate logs to them. A non-voter has nothing to do other than receiving logs from the leader. DFS Example \u00b6 FQ: DatafuseQuery node flight: arrow-flight server handler: execution handler DFS: distributed FS FS: local FS L: leader of raft for meta F: follower of raft lnr: learner FQ | | 1. put block | or create table | -----------------|------------------------------------- v flight <-----. flight | | | 2. | 8. pull block 5. commit | | meta v | .-------handler '- handler | | | ^ | | 3. 9. | | | v v | | DFS DFS| | | | | | | 4. 10. | | | v v | | FS FS | 7. notify handler | | of meta changes v 6. meta bcast | meta : L----+-------+------+------|-+-------. `->F `->F `->F | `->F `->lnr `-' nodes: N1 N2 N3 N4 N5 N6 ... Table format \u00b6 A table in IFileSystem consists of several files and the structure is similar to a append-only log: Table head: contains schema and a pointer to the latest manifest file. The table head must be updated atomically . Manifest: describes what data blocks belongs to a table. There is a list of data block files pointing to the latest updates, and a pointer to previous manifest i.e., the last version this update based on. Data: a collection of operation logs. T: table head Mi: manifest di: data block files T | v M0 <---- M1 <----- M2 | | | `-> d0 +-> d1 +-> d3 | | `-> d2 `-> d4 A typical update workflow would be like the following: Handler receives a batch of updates, including inserts and deletes. It writes to one or more data blocks, with some unique keys. Handler reads the latest table head file, finds out the latest manifest, e.g., M1, and compose a new manifest e.g., M2, containing the data block keys from step-1 and a pointer to M1. Write it to DFS with a unique key. Handler atomically update the table head file to change the latest manifest pointer to M2. If race encountered, retry from step-2. The challenge is on step-3: An IFileSystem impl may not support atomic update. In this case, write the table head file with a mono-incremental key, e.g. t-001 , then t-002 ... And a read operation should list all of the heads and find out the latest version(resulting in eventual consistency). This strategy only requires atomic-add operation, i.e., put-if-absent. Atomic-add can be done on behalf of the meta cluster, since it is an easy job for a raft group or any other consensus group to generate mono-incremental ids. Table compaction \u00b6 Compaction merges several earliest manifest into one, and optionally merges overlapping data blocks. Compaction generates a new manifest e.g., M1' from M1. Then it removes M0 and M1. A reading process should try to read both M1 and M1', and use either one it sees. A manifest or data block will be added or removed, but never updated, since in a distributed system updating data results in super complicated consistency challenges . T | v M0 <---- M1 <----+ M2 | | | | `-> d0 +-> d1 | +-> d3 | | | `-> d2 | `-> d4 | | | | M1' <---' +-> d0 | +-> d1 | `-> d2","title":"DatafuseStore Design"},{"location":"rfcs/store/0001-store-design/#ifilesystem","text":"IFileSystem defines an abstract storage layer that DatafuseStore would runs on. An IFileSystem impl in the cluster is the only stateful component. Local FS: impl IFileSystem API and use a local disk folder as storage. Suitable for a single node DatafuseQuery deployment. DFS: impl IFileSystem and setup an aws-S3 like storage service. A DFS organizes multiple LocalFS with a centralized meta data service. Object Storage Adapters: an IFileSystem impl that builds upon an object storage service on cloud. IFileSystem defines following API-s: add : AKA put-if-absent: add a file only if it is absent. read_all : read all bytes of a file. list : retrieve a list of files with specified prefix. TODO","title":"IFileSystem"},{"location":"rfcs/store/0001-store-design/#api","text":"DatafuseQuery and DatafuseStore talks arrow-flight protocol. Schema related operations such as create table or create database are wrapped with a FlightService::do_action RPC. Data operation such as reading or writing a block are done with FlightService::do_get and FlightService::do_put . See common/flights/src/store_client.rs .","title":"API"},{"location":"rfcs/store/0001-store-design/#dfs","text":"The most important component in DatafuseStore is the DFS. DFS mainly consists of two parts: the meta data cluster and block storage cluster. Block cluster is unaware of data placement and is purely a set of servers providing object like write and read API. Data placement is part of the meta data and is stored in the meta cluster.","title":"DFS"},{"location":"rfcs/store/0001-store-design/#meta-data-cluster","text":"All meta(the cluster meta and file keys) has a copy that resides in memory on every node for instant access. Every update to meta is done by committing a raft log of the updated entry into meta cluster. A cluster keeps its meta data in a raft group with typically 5 candidate nodes and all other nodes as learners. Candidate nodes provides meta data read and write API. Every other node is a learner , which does not elect but just subscribes meta change message from the 5 candidates.","title":"Meta data cluster"},{"location":"rfcs/store/0001-store-design/#in-process-metadata-components","text":"A DatafuseStore process includes two grpc API: the flight service and the meta service. Meta related components are wrapped into MetaNode , in which a Raft instance is maintained along with storage and network engines. MetaNode is the only entry for other DatafuseStore components to access meta data. RaftNode communicates with remote RaftNode through Network , which send messages to meta-grpc service on other DatafuseStore nodes. Network relies on Storage to find out info of other nodes. DatafuseStore components: .---------------------------. | | | flight-grpc meta-grpc | | | | | | '--. .-----' | | v v | | MetaNode | | | | | | | v | | | RaftNode | | | .--' | | | v v v | | Storage <- Network | | | '---------------------------'","title":"In-process metadata components"},{"location":"rfcs/store/0001-store-design/#meta-data-structure","text":"Meta data includes hardware information: nodes, the file information: keys and data placement information: slots. message Meta { map < string , string > keys ; repeated Slot slots ; map < int64 , Node > nodes } message Node { int64 NodeId repeated string Addresses } message Slot { repeated int64 node_ids ; }","title":"Meta data structure"},{"location":"rfcs/store/0001-store-design/#file-format","text":"A data block in DFS or local-FS is a complete Parquet data, with schema embedded. The schema in block should always be identical to the schema stored in table-meta. Otherwise it is a severe bug. Parquet has its own segmentation and index in it which is similar to ClickHouse file structure. See: https://parquet.apache.org/documentation/latest/ A schema file such as table or database is in protobuf format.","title":"File format"},{"location":"rfcs/store/0001-store-design/#data-placement","text":"A file in DFS has 3 copies. A file is identified with a unique key . Every key is assigned to a virtual allocation unit slot by some hash algo. A slot is assigned to 3 nodes. The slot-to-nodes mapping is part of the DFS meta data.","title":"Data placement"},{"location":"rfcs/store/0001-store-design/#replication","text":"Once a data block is persisted on local fs and the corresponding meta data is committed, DFS ack the client an OK message. Every node is either a follower or leaner of the meta data raft group thus will receive the meta changes. If a node found that a key is uploaded and the slot for the key is on this node, it pulls the data block from the uploading node.","title":"Replication"},{"location":"rfcs/store/0001-store-design/#meta-cluster-startup","text":"A DatafuseStore cluster is stateful thus the startup is done in several steps: Boot up the first node in a cluster, by calling MetaNode::boot() . This func creates an empty raft instance add initialize itself as the leader of the solo cluster. Creating node is done with MetaNode::boot_non_voter() . This func does nothing more than initializing a raft instance. It returns the MetaNode instance with network API ready for accepting raft communications. At this point it is unaware of anything about the cluster. To add this new node to a cluster, send an Cmd::AddNode request to the leader. The leader will then commit the node info into its own storage and start sending raft snapshot and logs to the new node. When a node is shutting down and restarted, just the MetaNode::new() is used to start the meta-service. When a node is restarted: A candidate(AKA voter) that becomes the new leader is able to find out every node from its local storage and then add them as non-voter in order to replicate logs to them. A non-voter has nothing to do other than receiving logs from the leader.","title":"Meta cluster startup"},{"location":"rfcs/store/0001-store-design/#dfs-example","text":"FQ: DatafuseQuery node flight: arrow-flight server handler: execution handler DFS: distributed FS FS: local FS L: leader of raft for meta F: follower of raft lnr: learner FQ | | 1. put block | or create table | -----------------|------------------------------------- v flight <-----. flight | | | 2. | 8. pull block 5. commit | | meta v | .-------handler '- handler | | | ^ | | 3. 9. | | | v v | | DFS DFS| | | | | | | 4. 10. | | | v v | | FS FS | 7. notify handler | | of meta changes v 6. meta bcast | meta : L----+-------+------+------|-+-------. `->F `->F `->F | `->F `->lnr `-' nodes: N1 N2 N3 N4 N5 N6 ...","title":"DFS Example"},{"location":"rfcs/store/0001-store-design/#table-format","text":"A table in IFileSystem consists of several files and the structure is similar to a append-only log: Table head: contains schema and a pointer to the latest manifest file. The table head must be updated atomically . Manifest: describes what data blocks belongs to a table. There is a list of data block files pointing to the latest updates, and a pointer to previous manifest i.e., the last version this update based on. Data: a collection of operation logs. T: table head Mi: manifest di: data block files T | v M0 <---- M1 <----- M2 | | | `-> d0 +-> d1 +-> d3 | | `-> d2 `-> d4 A typical update workflow would be like the following: Handler receives a batch of updates, including inserts and deletes. It writes to one or more data blocks, with some unique keys. Handler reads the latest table head file, finds out the latest manifest, e.g., M1, and compose a new manifest e.g., M2, containing the data block keys from step-1 and a pointer to M1. Write it to DFS with a unique key. Handler atomically update the table head file to change the latest manifest pointer to M2. If race encountered, retry from step-2. The challenge is on step-3: An IFileSystem impl may not support atomic update. In this case, write the table head file with a mono-incremental key, e.g. t-001 , then t-002 ... And a read operation should list all of the heads and find out the latest version(resulting in eventual consistency). This strategy only requires atomic-add operation, i.e., put-if-absent. Atomic-add can be done on behalf of the meta cluster, since it is an easy job for a raft group or any other consensus group to generate mono-incremental ids.","title":"Table format"},{"location":"rfcs/store/0001-store-design/#table-compaction","text":"Compaction merges several earliest manifest into one, and optionally merges overlapping data blocks. Compaction generates a new manifest e.g., M1' from M1. Then it removes M0 and M1. A reading process should try to read both M1 and M1', and use either one it sees. A manifest or data block will be added or removed, but never updated, since in a distributed system updating data results in super complicated consistency challenges . T | v M0 <---- M1 <----+ M2 | | | | `-> d0 +-> d1 | +-> d3 | | | `-> d2 | `-> d4 | | | | M1' <---' +-> d0 | +-> d1 | `-> d2","title":"Table compaction"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/","text":"Calculates the arg value for a maximum val value. If there are several different values of arg for maximum values of val , returns the first of these values encountered. Syntax \u00b6 argMax(arg, val) Arguments \u00b6 Arguments Description arg Argument val Value Return Type \u00b6 arg value that corresponds to maximum val value. matches arg type. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMax(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMax(user, salary) | +----------------------+ | 0 | +----------------------+","title":"argMax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#syntax","text":"argMax(arg, val)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#arguments","text":"Arguments Description arg Argument val Value","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#return-type","text":"arg value that corresponds to maximum val value. matches arg type.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-argmax/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMax(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMax(user, salary) | +----------------------+ | 0 | +----------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/","text":"Calculates the arg value for a minimum val value. If there are several different values of arg for minimum values of val , returns the first of these values encountered. Syntax \u00b6 argMin(arg, val) Arguments \u00b6 Arguments Description arg Argument val Value Return Type \u00b6 arg value that corresponds to minimum val value. matches arg type. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMin(user, salary) | +----------------------+ | 1 | +----------------------+","title":"argMin"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#syntax","text":"argMin(arg, val)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#arguments","text":"Arguments Description arg Argument val Value","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#return-type","text":"arg value that corresponds to minimum val value. matches arg type.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-argmin/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. Input table: SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user ORDER BY salary ASC; +----------+------+ | salary | user | +----------+------+ | 16661667 | 1 | | 16665000 | 2 | | 16668333 | 0 | +----------+------+ mysql> SELECT argMin(user, salary) FROM (SELECT sum(number) AS salary, number%3 AS user FROM numbers_mt(10000) GROUP BY user); +----------------------+ | argMin(user, salary) | +----------------------+ | 1 | +----------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-avg-if/","text":"avgIf \u00b6 The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. avgIf(column, cond) Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT avg(number) FROM numbers(10); +-------------+ | avg(number) | +-------------+ | 4.5 | +-------------+ mysql> SELECT avgIf(number, number > 7) FROM numbers(10); +-----------------------------+ | avgIf(number, (number > 7)) | +-----------------------------+ | 8.5 | +-----------------------------+","title":"avgIf"},{"location":"sqlstatement/aggregate-functions/aggregate-avg-if/#avgif","text":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. avgIf(column, cond)","title":"avgIf"},{"location":"sqlstatement/aggregate-functions/aggregate-avg-if/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT avg(number) FROM numbers(10); +-------------+ | avg(number) | +-------------+ | 4.5 | +-------------+ mysql> SELECT avgIf(number, number > 7) FROM numbers(10); +-----------------------------+ | avgIf(number, (number > 7)) | +-----------------------------+ | 8.5 | +-----------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/","text":"Aggregate function. The AVG() function returns the average value of an expression. Note: NULL values are not counted. Syntax \u00b6 AVG ( expression ) Arguments \u00b6 Arguments Description expression Any numerical expression Return Type \u00b6 double Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT AVG(*) FROM numbers(3); +--------+ | avg(*) | +--------+ | 1 | +--------+ mysql> SELECT AVG(number) FROM numbers(3); +-------------+ | avg(number) | +-------------+ | 1 | +-------------+ mysql> SELECT AVG(number+1) FROM numbers(3); +----------------------+ | avg(plus(number, 1)) | +----------------------+ | 2 | +----------------------+ mysql> SELECT AVG(number+1) AS a FROM numbers(3); +------+ | a | +------+ | 2 | +------+","title":"AVG"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#syntax","text":"AVG ( expression )","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#arguments","text":"Arguments Description expression Any numerical expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#return-type","text":"double","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-avg/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT AVG(*) FROM numbers(3); +--------+ | avg(*) | +--------+ | 1 | +--------+ mysql> SELECT AVG(number) FROM numbers(3); +-------------+ | avg(number) | +-------------+ | 1 | +-------------+ mysql> SELECT AVG(number+1) FROM numbers(3); +----------------------+ | avg(plus(number, 1)) | +----------------------+ | 2 | +----------------------+ mysql> SELECT AVG(number+1) AS a FROM numbers(3); +------+ | a | +------+ | 2 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-combinator/","text":"Aggregate Function Combinators \u00b6 The name of an aggregate function can have a suffix appended to it. This changes the way the aggregate function works. Distinct \u00b6 Every unique combination of arguments will be aggregated only once. Repeating values are ignored. count(distinct(expression)) sum(distinct(expression)) min(distinct(expression)) max(distinct(expression)) Examples \u00b6 mysql> SELECT sum(distinct(number%3)) FROM numbers_mt(10); +----------------------------+ | sum(distinct (number % 3)) | +----------------------------+ | 3 | +----------------------------+","title":"DISTINCT"},{"location":"sqlstatement/aggregate-functions/aggregate-combinator/#aggregate-function-combinators","text":"The name of an aggregate function can have a suffix appended to it. This changes the way the aggregate function works.","title":"Aggregate Function Combinators"},{"location":"sqlstatement/aggregate-functions/aggregate-combinator/#distinct","text":"Every unique combination of arguments will be aggregated only once. Repeating values are ignored. count(distinct(expression)) sum(distinct(expression)) min(distinct(expression)) max(distinct(expression))","title":"Distinct"},{"location":"sqlstatement/aggregate-functions/aggregate-combinator/#examples","text":"mysql> SELECT sum(distinct(number%3)) FROM numbers_mt(10); +----------------------------+ | sum(distinct (number % 3)) | +----------------------------+ | 3 | +----------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-count-distinct/","text":"Aggregate function. The count(distinct ...) function calculates the uniq value of a set of values. Note: NULL values are not counted. Syntax \u00b6 COUNT(distinct arguments ...) UNIQ(arguments) Arguments \u00b6 Arguments Description expression Any expression, size of the arguments is [1, 32] Return Type \u00b6 UInt64 Examples \u00b6 mysql> SELECT count(distinct number % 3) FROM numbers(1000); +------------------------------+ | count(distinct (number % 3)) | +------------------------------+ | 3 | +------------------------------+ mysql> SELECT uniq(number % 3, number) FROM numbers(1000); +----------------------------+ | uniq((number % 3), number) | +----------------------------+ | 1000 | +----------------------------+ 1 row in set (0.02 sec) mysql> SELECT uniq(number % 3, number) = count(distinct number %3, number) FROM numbers(1000); +---------------------------------------------------------------------+ | (uniq((number % 3), number) = count(distinct (number % 3), number)) | +---------------------------------------------------------------------+ | true | +---------------------------------------------------------------------+ 1 row in set (0.03 sec","title":"countDistinct"},{"location":"sqlstatement/aggregate-functions/aggregate-count-distinct/#syntax","text":"COUNT(distinct arguments ...) UNIQ(arguments)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-count-distinct/#arguments","text":"Arguments Description expression Any expression, size of the arguments is [1, 32]","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-count-distinct/#return-type","text":"UInt64","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-count-distinct/#examples","text":"mysql> SELECT count(distinct number % 3) FROM numbers(1000); +------------------------------+ | count(distinct (number % 3)) | +------------------------------+ | 3 | +------------------------------+ mysql> SELECT uniq(number % 3, number) FROM numbers(1000); +----------------------------+ | uniq((number % 3), number) | +----------------------------+ | 1000 | +----------------------------+ 1 row in set (0.02 sec) mysql> SELECT uniq(number % 3, number) = count(distinct number %3, number) FROM numbers(1000); +---------------------------------------------------------------------+ | (uniq((number % 3), number) = count(distinct (number % 3), number)) | +---------------------------------------------------------------------+ | true | +---------------------------------------------------------------------+ 1 row in set (0.03 sec","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-count-if/","text":"countIf \u00b6 The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. countIf(column, cond) Examples \u00b6 Note numbers_mt(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(number) FROM numbers(10); +---------------+ | count(number) | +---------------+ | 10 | +---------------+ mysql> SELECT countIf(number, number > 7) FROM numbers(10); +-------------------------------+ | countIf(number, (number > 7)) | +-------------------------------+ | 2 | +-------------------------------+","title":"countIf"},{"location":"sqlstatement/aggregate-functions/aggregate-count-if/#countif","text":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. countIf(column, cond)","title":"countIf"},{"location":"sqlstatement/aggregate-functions/aggregate-count-if/#examples","text":"Note numbers_mt(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(number) FROM numbers(10); +---------------+ | count(number) | +---------------+ | 10 | +---------------+ mysql> SELECT countIf(number, number > 7) FROM numbers(10); +-------------------------------+ | countIf(number, (number > 7)) | +-------------------------------+ | 2 | +-------------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-count/","text":"Aggregate function. The COUNT() function returns the number of records returned by a select query. Note: NULL values are not counted. Syntax \u00b6 COUNT(expression) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. * is also allowed, to indicate pure row counting. Return Type \u00b6 An integer. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(*) FROM numbers(3); +----------+ | count(*) | +----------+ | 3 | +----------+ mysql> SELECT count(number) FROM numbers(3); +---------------+ | count(number) | +---------------+ | 3 | +---------------+ mysql> SELECT count(number) AS c FROM numbers(3); +------+ | c | +------+ | 3 | +------+","title":"COUNT"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#syntax","text":"COUNT(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. * is also allowed, to indicate pure row counting.","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#return-type","text":"An integer.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-count/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT count(*) FROM numbers(3); +----------+ | count(*) | +----------+ | 3 | +----------+ mysql> SELECT count(number) FROM numbers(3); +---------------+ | count(number) | +---------------+ | 3 | +---------------+ mysql> SELECT count(number) AS c FROM numbers(3); +------+ | c | +------+ | 3 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-max-if/","text":"maxIf \u00b6 The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. maxIf(column, cond) Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT max(number) FROM numbers(10); +-------------+ | max(number) | +-------------+ | 9 | +-------------+ mysql> SELECT maxIf(number, number < 7) FROM numbers(10); +-----------------------------+ | maxIf(number, (number < 7)) | +-----------------------------+ | 6 | +-----------------------------+","title":"maxIf"},{"location":"sqlstatement/aggregate-functions/aggregate-max-if/#maxif","text":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. maxIf(column, cond)","title":"maxIf"},{"location":"sqlstatement/aggregate-functions/aggregate-max-if/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT max(number) FROM numbers(10); +-------------+ | max(number) | +-------------+ | 9 | +-------------+ mysql> SELECT maxIf(number, number < 7) FROM numbers(10); +-----------------------------+ | maxIf(number, (number < 7)) | +-----------------------------+ | 6 | +-----------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-max/","text":"Aggregate function. The MAX() function returns the maximum value in a set of values. Syntax \u00b6 MAX(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 The maximum value, in the type of the value. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MAX(*) FROM numbers(3); +--------+ | max(*) | +--------+ | 2 | +--------+ mysql> SELECT MAX(number) FROM numbers(3); +-------------+ | max(number) | +-------------+ | 2 | +-------------+ mysql> SELECT MAX(number) AS max FROM numbers(3); +------+ | max | +------+ | 2 | +------+","title":"MAX"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#syntax","text":"MAX(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#return-type","text":"The maximum value, in the type of the value.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-max/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MAX(*) FROM numbers(3); +--------+ | max(*) | +--------+ | 2 | +--------+ mysql> SELECT MAX(number) FROM numbers(3); +-------------+ | max(number) | +-------------+ | 2 | +-------------+ mysql> SELECT MAX(number) AS max FROM numbers(3); +------+ | max | +------+ | 2 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-min-if/","text":"minIf \u00b6 The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. minIf(column, cond) Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT min(number) FROM numbers(10); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT minIf(number, number > 7) FROM numbers(10); +-----------------------------+ | minIf(number, (number > 7)) | +-----------------------------+ | 8 | +-----------------------------+","title":"minIf"},{"location":"sqlstatement/aggregate-functions/aggregate-min-if/#minif","text":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. minIf(column, cond)","title":"minIf"},{"location":"sqlstatement/aggregate-functions/aggregate-min-if/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT min(number) FROM numbers(10); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT minIf(number, number > 7) FROM numbers(10); +-----------------------------+ | minIf(number, (number > 7)) | +-----------------------------+ | 8 | +-----------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-min/","text":"Aggregate function. The MIN() function returns the minimum value in a set of values. Syntax \u00b6 MIN(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 The minimum value, in the type of the value. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MIN(*) FROM numbers(3); +--------+ | min(*) | +--------+ | 0 | +--------+ mysql> SELECT MIN(number) FROM numbers(3); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT MIN(number) AS min FROM numbers(3); +------+ | min | +------+ | 0 | +------+","title":"MIN"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#syntax","text":"MIN(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#return-type","text":"The minimum value, in the type of the value.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-min/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT MIN(*) FROM numbers(3); +--------+ | min(*) | +--------+ | 0 | +--------+ mysql> SELECT MIN(number) FROM numbers(3); +-------------+ | min(number) | +-------------+ | 0 | +-------------+ mysql> SELECT MIN(number) AS min FROM numbers(3); +------+ | min | +------+ | 0 | +------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-sum-if/","text":"sumIf \u00b6 The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. sumIf(column, cond) Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT sum(number) FROM numbers(10); +-------------+ | sum(number) | +-------------+ | 45 | +-------------+ mysql> SELECT sumIf(number, number > 7) FROM numbers(10); +-----------------------------+ | sumIf(number, (number > 7)) | +-----------------------------+ | 17 | +-----------------------------+","title":"sumIf"},{"location":"sqlstatement/aggregate-functions/aggregate-sum-if/#sumif","text":"The suffix -If can be appended to the name of any aggregate function. In this case, the aggregate function accepts an extra argument \u2013 a condition. sumIf(column, cond)","title":"sumIf"},{"location":"sqlstatement/aggregate-functions/aggregate-sum-if/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT sum(number) FROM numbers(10); +-------------+ | sum(number) | +-------------+ | 45 | +-------------+ mysql> SELECT sumIf(number, number > 7) FROM numbers(10); +-----------------------------+ | sumIf(number, (number > 7)) | +-----------------------------+ | 17 | +-----------------------------+","title":"Examples"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/","text":"Aggregate function. The SUM() function calculates the sum of a set of values. Note: NULL values are not counted. Syntax \u00b6 SUM(expression) Arguments \u00b6 Arguments Description expression Any expression Return Type \u00b6 A double if the input type is double, otherwise integer. Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT SUM(*) FROM numbers(3); +--------+ | sum(*) | +--------+ | 3 | +--------+ mysql> SELECT SUM(number) FROM numbers(3); +-------------+ | sum(number) | +-------------+ | 3 | +-------------+ mysql> SELECT SUM(number) AS sum FROM numbers(3); +------+ | sum | +------+ | 3 | +------+ mysql> SELECT SUM(number+2) AS sum FROM numbers(3); +------+ | sum | +------+ | 9 | +------+","title":"SUM"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#syntax","text":"SUM(expression)","title":"Syntax"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#arguments","text":"Arguments Description expression Any expression","title":"Arguments"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#return-type","text":"A double if the input type is double, otherwise integer.","title":"Return Type"},{"location":"sqlstatement/aggregate-functions/aggregate-sum/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SELECT SUM(*) FROM numbers(3); +--------+ | sum(*) | +--------+ | 3 | +--------+ mysql> SELECT SUM(number) FROM numbers(3); +-------------+ | sum(number) | +-------------+ | 3 | +-------------+ mysql> SELECT SUM(number) AS sum FROM numbers(3); +------+ | sum | +------+ | 3 | +------+ mysql> SELECT SUM(number+2) AS sum FROM numbers(3); +------+ | sum | +------+ | 9 | +------+","title":"Examples"},{"location":"sqlstatement/conversion-functions/cast/","text":"Convert a value from one data type to another data type. Syntax \u00b6 CAST ( x AS t ) Arguments \u00b6 Arguments Description x A value to convert. t The target data type. Return Type \u00b6 Converted value. Examples \u00b6 mysql> SELECT CAST(1 AS VARCHAR); +-----------------+ | cast(1 as Utf8) | +-----------------+ | 1 | +-----------------+ mysql> SELECT CAST(1 AS UInt64); +-------------------+ | cast(1 as UInt64) | +-------------------+ | 1 | +-------------------+ mysql> SELECT toTypeName(CAST(1 AS UInt64)); +-------------------------------+ | toTypeName(cast(1 as UInt64)) | +-------------------------------+ | UInt64 | +-------------------------------+","title":"CAST"},{"location":"sqlstatement/conversion-functions/cast/#syntax","text":"CAST ( x AS t )","title":"Syntax"},{"location":"sqlstatement/conversion-functions/cast/#arguments","text":"Arguments Description x A value to convert. t The target data type.","title":"Arguments"},{"location":"sqlstatement/conversion-functions/cast/#return-type","text":"Converted value.","title":"Return Type"},{"location":"sqlstatement/conversion-functions/cast/#examples","text":"mysql> SELECT CAST(1 AS VARCHAR); +-----------------+ | cast(1 as Utf8) | +-----------------+ | 1 | +-----------------+ mysql> SELECT CAST(1 AS UInt64); +-------------------+ | cast(1 as UInt64) | +-------------------+ | 1 | +-------------------+ mysql> SELECT toTypeName(CAST(1 AS UInt64)); +-------------------------------+ | toTypeName(cast(1 as UInt64)) | +-------------------------------+ | UInt64 | +-------------------------------+","title":"Examples"},{"location":"sqlstatement/conversion-functions/type-conversion/","text":"toInt(8|16|32|64) Syntax \u00b6 toInt8 ( expr ) \u2014 Results in the Int8 data type . toInt16 ( expr ) \u2014 Results in the Int16 data type . toInt32 ( expr ) \u2014 Results in the Int32 data type . toInt64 ( expr ) \u2014 Results in the Int64 data type . Examples \u00b6 mysql> SELECT toInt8(1); +-----------+ | toInt8(1) | +-----------+ | 1 | +-----------+ mysql> SELECT toTypeName(toInt8(1)); +-----------------------+ | toTypeName(toInt8(1)) | +-----------------------+ | Int8 | +-----------------------+","title":"Type Conversion"},{"location":"sqlstatement/conversion-functions/type-conversion/#syntax","text":"toInt8 ( expr ) \u2014 Results in the Int8 data type . toInt16 ( expr ) \u2014 Results in the Int16 data type . toInt32 ( expr ) \u2014 Results in the Int32 data type . toInt64 ( expr ) \u2014 Results in the Int64 data type .","title":"Syntax"},{"location":"sqlstatement/conversion-functions/type-conversion/#examples","text":"mysql> SELECT toInt8(1); +-----------+ | toInt8(1) | +-----------+ | 1 | +-----------+ mysql> SELECT toTypeName(toInt8(1)); +-----------------------+ | toTypeName(toInt8(1)) | +-----------------------+ | Int8 | +-----------------------+","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/","text":"Create a database. Syntax \u00b6 CREATE DATABASE < database_name > Examples \u00b6 mysql > CREATE DATABASE test ;","title":"CREATE DATABASE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/#syntax","text":"CREATE DATABASE < database_name >","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-database/#examples","text":"mysql > CREATE DATABASE test ;","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-table/","text":"Create a new table. Syntax \u00b6 CREATE TABLE [ IF NOT EXISTS ] [ db .] table_name ( name1 type1 , name2 type2 , ... ) ENGINE = engine Note Local engine is one of Memory , Parquet , JSONEachRow , Null or CSV , data will be stored in the DatafuseQuery memory/disk locally. Remote engine is remote , will be stored in the remote DatafuseStore cluster. Examples \u00b6 Memory engine \u00b6 mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > SELECT * FROM test ; + ------+---------+ | a | b | + ------+---------+ | 888 | stars | + ------+---------+","title":"CREATE TABLE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-table/#syntax","text":"CREATE TABLE [ IF NOT EXISTS ] [ db .] table_name ( name1 type1 , name2 type2 , ... ) ENGINE = engine Note Local engine is one of Memory , Parquet , JSONEachRow , Null or CSV , data will be stored in the DatafuseQuery memory/disk locally. Remote engine is remote , will be stored in the remote DatafuseStore cluster.","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-table/#examples","text":"","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-create-table/#memory-engine","text":"mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > SELECT * FROM test ; + ------+---------+ | a | b | + ------+---------+ | 888 | stars | + ------+---------+","title":"Memory engine"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/","text":"Drop a database. Syntax \u00b6 DROP DATABASE [ IF EXISTS ] < database_name > Examples \u00b6 mysql > DROP DATABASE test ;","title":"DROP DATABASE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/#syntax","text":"DROP DATABASE [ IF EXISTS ] < database_name >","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-database/#examples","text":"mysql > DROP DATABASE test ;","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-table/","text":"Deletes the table. Syntax \u00b6 DROP TABLE [ IF EXISTS ] [ db .] name Examples \u00b6 mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > DROP TABLE test ;","title":"DROP TABLE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-table/#syntax","text":"DROP TABLE [ IF EXISTS ] [ db .] name","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-drop-table/#examples","text":"mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > DROP TABLE test ;","title":"Examples"},{"location":"sqlstatement/data-definition-language-ddl/ddl-truncate-table/","text":"Empties the table completely. Syntax \u00b6 TRUNCATE TABLE [ db .] name Examples \u00b6 mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > SELECT * FROM test ; + ------+---------+ | a | b | + ------+---------+ | 888 | stars | + ------+---------+ mysql > TRUNCATE TABLE test ; mysql > SELECT * FROM test ;","title":"TRUNCATE TABLE"},{"location":"sqlstatement/data-definition-language-ddl/ddl-truncate-table/#syntax","text":"TRUNCATE TABLE [ db .] name","title":"Syntax"},{"location":"sqlstatement/data-definition-language-ddl/ddl-truncate-table/#examples","text":"mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > SELECT * FROM test ; + ------+---------+ | a | b | + ------+---------+ | 888 | stars | + ------+---------+ mysql > TRUNCATE TABLE test ; mysql > SELECT * FROM test ;","title":"Examples"},{"location":"sqlstatement/data-manipulation-language-dml/dml-insert/","text":"Writing data. Syntax \u00b6 INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ... Note Local engine is one of Memory , Parquet , JSONEachRow , Null or CSV , data will be stored in the DatafuseQuery memory/disk locally. Remote engine is remote , will be stored in the remote DatafuseStore cluster. Examples \u00b6 Memory engine \u00b6 mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > INSERT INTO test values ( 1024 , 'stars' ); mysql > SELECT * FROM test ; + ------+-------+ | a | b | + ------+-------+ | 888 | stars | | 1024 | stars | + ------+-------+","title":"INSERT"},{"location":"sqlstatement/data-manipulation-language-dml/dml-insert/#syntax","text":"INSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ... Note Local engine is one of Memory , Parquet , JSONEachRow , Null or CSV , data will be stored in the DatafuseQuery memory/disk locally. Remote engine is remote , will be stored in the remote DatafuseStore cluster.","title":"Syntax"},{"location":"sqlstatement/data-manipulation-language-dml/dml-insert/#examples","text":"","title":"Examples"},{"location":"sqlstatement/data-manipulation-language-dml/dml-insert/#memory-engine","text":"mysql > CREATE TABLE test ( a UInt64 , b Varchar ) Engine = Memory ; mysql > INSERT INTO test ( a , b ) values ( 888 , 'stars' ); mysql > INSERT INTO test values ( 1024 , 'stars' ); mysql > SELECT * FROM test ; + ------+-------+ | a | b | + ------+-------+ | 888 | stars | | 1024 | stars | + ------+-------+","title":"Memory engine"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/","text":"Retrieves data from a table. Syntax \u00b6 SELECT [ALL | DISTINCT] select_expr [[AS] alias], ... [INTO variable [, ...]] [ FROM table_references [WHERE expr] [GROUP BY {{col_name | expr | position}, ... | extended_grouping_expr}] [HAVING expr] [ORDER BY {col_name | expr} [ASC | DESC], ...] [LIMIT row_count] ] Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. SELECT clause \u00b6 mysql> SELECT number FROM numbers(3); +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+ FROM clause \u00b6 mysql> SELECT number FROM numbers(3) AS a; +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+ WHERE clause \u00b6 mysql> SELECT number FROM numbers(3) WHERE number > 1; +--------+ | number | +--------+ | 2 | +--------+ 1 row in set (0.00 sec) GROUP BY clause \u00b6 mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) FROM numbers(10000) GROUP BY c1, c2; +------+------+-------------+ | c1 | c2 | MAX(number) | +------+------+-------------+ | 1 | 2 | 9995 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | | 0 | 1 | 9994 | | 0 | 0 | 9996 | | 1 | 0 | 9999 | +------+------+-------------+ 6 rows in set (0.00 sec) HAVING clause \u00b6 mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) as max FROM numbers(10000) GROUP BY c1, c2 HAVING max>9996; +------+------+------+ | c1 | c2 | max | +------+------+------+ | 1 | 0 | 9999 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | +------+------+------+ 3 rows in set (0.00 sec) ORDER By clause \u00b6 mysql> SELECT number FROM numbers(5) ORDER BY number ASC; +--------+ | number | +--------+ | 0 | | 1 | | 2 | | 3 | | 4 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number FROM numbers(5) ORDER BY number DESC; +--------+ | number | +--------+ | 4 | | 3 | | 2 | | 1 | | 0 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number%2 AS c1, number%3 AS c2 FROM numbers(5) ORDER BY c1 ASC, c2 DESC; +------+------+ | c1 | c2 | +------+------+ | 0 | 2 | | 0 | 1 | | 0 | 0 | | 1 | 1 | | 1 | 0 | +------+------+ 5 rows in set (0.00 sec) LIMIT clause \u00b6 mysql> SELECT number FROM numbers(1000000000) LIMIT 1; +--------+ | number | +--------+ | 0 | +--------+ 1 row in set (0.00 sec) Nested Sub-Selects \u00b6 SELECT statements can be nested in queries. SELECT ... [SELECT ...[SELECT [...]]] mysql> SELECT MIN(number) FROM (SELECT number%3 AS number FROM numbers(10)) GROUP BY number%2; +-------------+ | min(number) | +-------------+ | 1 | | 0 | +-------------+","title":"SELECT"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#syntax","text":"SELECT [ALL | DISTINCT] select_expr [[AS] alias], ... [INTO variable [, ...]] [ FROM table_references [WHERE expr] [GROUP BY {{col_name | expr | position}, ... | extended_grouping_expr}] [HAVING expr] [ORDER BY {col_name | expr} [ASC | DESC], ...] [LIMIT row_count] ] Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1.","title":"Syntax"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#select-clause","text":"mysql> SELECT number FROM numbers(3); +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+","title":"SELECT clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#from-clause","text":"mysql> SELECT number FROM numbers(3) AS a; +--------+ | number | +--------+ | 0 | | 1 | | 2 | +--------+","title":"FROM clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#where-clause","text":"mysql> SELECT number FROM numbers(3) WHERE number > 1; +--------+ | number | +--------+ | 2 | +--------+ 1 row in set (0.00 sec)","title":"WHERE clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#group-by-clause","text":"mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) FROM numbers(10000) GROUP BY c1, c2; +------+------+-------------+ | c1 | c2 | MAX(number) | +------+------+-------------+ | 1 | 2 | 9995 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | | 0 | 1 | 9994 | | 0 | 0 | 9996 | | 1 | 0 | 9999 | +------+------+-------------+ 6 rows in set (0.00 sec)","title":"GROUP BY clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#having-clause","text":"mysql> SELECT number%2 as c1, number%3 as c2, MAX(number) as max FROM numbers(10000) GROUP BY c1, c2 HAVING max>9996; +------+------+------+ | c1 | c2 | max | +------+------+------+ | 1 | 0 | 9999 | | 1 | 1 | 9997 | | 0 | 2 | 9998 | +------+------+------+ 3 rows in set (0.00 sec)","title":"HAVING clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#order-by-clause","text":"mysql> SELECT number FROM numbers(5) ORDER BY number ASC; +--------+ | number | +--------+ | 0 | | 1 | | 2 | | 3 | | 4 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number FROM numbers(5) ORDER BY number DESC; +--------+ | number | +--------+ | 4 | | 3 | | 2 | | 1 | | 0 | +--------+ 5 rows in set (0.00 sec) mysql> SELECT number%2 AS c1, number%3 AS c2 FROM numbers(5) ORDER BY c1 ASC, c2 DESC; +------+------+ | c1 | c2 | +------+------+ | 0 | 2 | | 0 | 1 | | 0 | 0 | | 1 | 1 | | 1 | 0 | +------+------+ 5 rows in set (0.00 sec)","title":"ORDER By clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#limit-clause","text":"mysql> SELECT number FROM numbers(1000000000) LIMIT 1; +--------+ | number | +--------+ | 0 | +--------+ 1 row in set (0.00 sec)","title":"LIMIT clause"},{"location":"sqlstatement/data-manipulation-language-dml/dml-select/#nested-sub-selects","text":"SELECT statements can be nested in queries. SELECT ... [SELECT ...[SELECT [...]]] mysql> SELECT MIN(number) FROM (SELECT number%3 AS number FROM numbers(10)) GROUP BY number%2; +-------------+ | min(number) | +-------------+ | 1 | | 0 | +-------------+","title":"Nested Sub-Selects"},{"location":"sqlstatement/data-types/data-type-integer-number/","text":"Data Type Size Min Value Max Value Int8 1 byte -128 127 Int16 2 byte -32768 32767 Int32 4 byte -2147483648 2147483647 Int64 8 byte -9223372036854775808 9223372036854775807 UInt8 1 byte 0 255 UInt16 2 byte 0 65535 UInt32 4 byte 0 4294967295 UInt64 8 byte 0 18446744073709551615","title":"Integer Numbers"},{"location":"sqlstatement/data-types/data-type-real-number/","text":"Data Type Size Precision Syntax Float32 4 byte 23 bits FLOAT Float64 8 byte 53 bits DOUBLE","title":"Real Numbers"},{"location":"sqlstatement/data-types/data-type-string-types/","text":"Strings of an arbitrary length. \u00b6 Data Type Syntax String Varchar","title":"String Types"},{"location":"sqlstatement/data-types/data-type-string-types/#strings-of-an-arbitrary-length","text":"Data Type Syntax String Varchar","title":"Strings of an arbitrary length."},{"location":"sqlstatement/describe-commands/describe-table/","text":"Displays information about the columns in a given table. Syntax \u00b6 DESC|DESCRIBE [database.]table_name Examples \u00b6 mysql> describe system.numbers; +--------+--------+------+ | Field | Type | Null | +--------+--------+------+ | number | UInt64 | NO | +--------+--------+------+ 1 row in set (0.01 sec)","title":"DESCRIBE TABLE"},{"location":"sqlstatement/describe-commands/describe-table/#syntax","text":"DESC|DESCRIBE [database.]table_name","title":"Syntax"},{"location":"sqlstatement/describe-commands/describe-table/#examples","text":"mysql> describe system.numbers; +--------+--------+------+ | Field | Type | Null | +--------+--------+------+ | number | UInt64 | NO | +--------+--------+------+ 1 row in set (0.01 sec)","title":"Examples"},{"location":"sqlstatement/hash-functions/siphash/","text":"Produces a 64-bit SipHash hash value. Syntax \u00b6 siphash ( expression ) siphash64 ( expression ) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. Return Type \u00b6 A UInt64 data type hash value. Examples \u00b6 mysql> SELECT SIPHASH('1234567890'); +---------------------+ | SIPHASH(1234567890) | +---------------------+ | 9027491583908826579 | +---------------------+ mysql> SELECT SIPHASH(1); +---------------------+ | SIPHASH(1) | +---------------------+ | 2206609067086327257 | +---------------------+ mysql> SELECT SIPHASH(1.2); +---------------------+ | SIPHASH(1.2) | +---------------------+ | 2854037594257667269 | +---------------------+ mysql> SELECT SIPHASH(number) FROM numbers(2); +----------------------+ | siphash(number) | +----------------------+ | 13646096770106105413 | | 2206609067086327257 | +----------------------+","title":"SIPHASH"},{"location":"sqlstatement/hash-functions/siphash/#syntax","text":"siphash ( expression ) siphash64 ( expression )","title":"Syntax"},{"location":"sqlstatement/hash-functions/siphash/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation.","title":"Arguments"},{"location":"sqlstatement/hash-functions/siphash/#return-type","text":"A UInt64 data type hash value.","title":"Return Type"},{"location":"sqlstatement/hash-functions/siphash/#examples","text":"mysql> SELECT SIPHASH('1234567890'); +---------------------+ | SIPHASH(1234567890) | +---------------------+ | 9027491583908826579 | +---------------------+ mysql> SELECT SIPHASH(1); +---------------------+ | SIPHASH(1) | +---------------------+ | 2206609067086327257 | +---------------------+ mysql> SELECT SIPHASH(1.2); +---------------------+ | SIPHASH(1.2) | +---------------------+ | 2854037594257667269 | +---------------------+ mysql> SELECT SIPHASH(number) FROM numbers(2); +----------------------+ | siphash(number) | +----------------------+ | 13646096770106105413 | | 2206609067086327257 | +----------------------+","title":"Examples"},{"location":"sqlstatement/information-functions/database/","text":"Returns the name of the currently selected database. If no database is selected, then this function returns default . Syntax \u00b6 SELECT DATABASE() Examples \u00b6 mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | default | +------------+","title":"DATABASE"},{"location":"sqlstatement/information-functions/database/#syntax","text":"SELECT DATABASE()","title":"Syntax"},{"location":"sqlstatement/information-functions/database/#examples","text":"mysql> SELECT DATABASE(); +------------+ | DATABASE() | +------------+ | default | +------------+","title":"Examples"},{"location":"sqlstatement/information-functions/version/","text":"Return the current version information of DatafuseQuery. Syntax \u00b6 SELECT VERSION() Examples \u00b6 mysql> SELECT VERSION(); +----------------------------------------------------------------------------------------+ | version() | +----------------------------------------------------------------------------------------+ | DatafuseQuery v-0.1.0-0f9ec31-simd(1.56.0-nightly-2021-08-10T15:25:36.875868571+00:00) | +----------------------------------------------------------------------------------------+","title":"VERSION"},{"location":"sqlstatement/information-functions/version/#syntax","text":"SELECT VERSION()","title":"Syntax"},{"location":"sqlstatement/information-functions/version/#examples","text":"mysql> SELECT VERSION(); +----------------------------------------------------------------------------------------+ | version() | +----------------------------------------------------------------------------------------+ | DatafuseQuery v-0.1.0-0f9ec31-simd(1.56.0-nightly-2021-08-10T15:25:36.875868571+00:00) | +----------------------------------------------------------------------------------------+","title":"Examples"},{"location":"sqlstatement/other-functions/totypename/","text":"ToTypeName function is used to return the name of a data type. Syntax \u00b6 ToTypeName ( expression ) Arguments \u00b6 Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation. Return Type \u00b6 String Examples \u00b6 mysql> SELECT ToTypeName(number) FROM numbers(2); +--------------------+ | ToTypeName(number) | +--------------------+ | UInt64 | | UInt64 | +--------------------+ mysql> SELECT ToTypeName(sum(number)) FROM numbers(2); +-------------------------+ | ToTypeName(sum(number)) | +-------------------------+ | UInt64 | +-------------------------+","title":"ToTypeName"},{"location":"sqlstatement/other-functions/totypename/#syntax","text":"ToTypeName ( expression )","title":"Syntax"},{"location":"sqlstatement/other-functions/totypename/#arguments","text":"Arguments Description expression Any expression. This may be a column name, the result of another function, or a math operation.","title":"Arguments"},{"location":"sqlstatement/other-functions/totypename/#return-type","text":"String","title":"Return Type"},{"location":"sqlstatement/other-functions/totypename/#examples","text":"mysql> SELECT ToTypeName(number) FROM numbers(2); +--------------------+ | ToTypeName(number) | +--------------------+ | UInt64 | | UInt64 | +--------------------+ mysql> SELECT ToTypeName(sum(number)) FROM numbers(2); +-------------------------+ | ToTypeName(sum(number)) | +-------------------------+ | UInt64 | +-------------------------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-create-table/","text":"Shows the CREATE TABLE statement that creates the named table. Syntax \u00b6 SHOW CREATE TABLE [database.]table_name Examples \u00b6 Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SHOW CREATE TABLE system.numbers; +---------+--------------------------------------------------------------------+ | Table | Create Table | +---------+--------------------------------------------------------------------+ | numbers | CREATE TABLE `numbers` ( `number` UInt64, ) ENGINE=SystemNumbers | +---------+--------------------------------------------------------------------+","title":"SHOW CREATE TABLE"},{"location":"sqlstatement/show-commands/show-create-table/#syntax","text":"SHOW CREATE TABLE [database.]table_name","title":"Syntax"},{"location":"sqlstatement/show-commands/show-create-table/#examples","text":"Note numbers(N) \u2013 A table for test with the single number column (UInt64) that contains integers from 0 to N-1. mysql> SHOW CREATE TABLE system.numbers; +---------+--------------------------------------------------------------------+ | Table | Create Table | +---------+--------------------------------------------------------------------+ | numbers | CREATE TABLE `numbers` ( `number` UInt64, ) ENGINE=SystemNumbers | +---------+--------------------------------------------------------------------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-databases/","text":"Shows the list of databases that exist on the instance. Syntax \u00b6 SHOW DATABASES Examples \u00b6 mysql> SHOW DATABASES; +----------+ | name | +----------+ | default | | for_test | | local | | system | | test | +----------+","title":"SHOW DATABASES"},{"location":"sqlstatement/show-commands/show-databases/#syntax","text":"SHOW DATABASES","title":"Syntax"},{"location":"sqlstatement/show-commands/show-databases/#examples","text":"mysql> SHOW DATABASES; +----------+ | name | +----------+ | default | | for_test | | local | | system | | test | +----------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-processlist/","text":"The Datafuse process list indicates the operations currently being performed by the set of threads executing within the server. The SHOW PROCESSLIST statement is one source of process information. Syntax \u00b6 SHOW PROCESSLIST Examples \u00b6 mysql> SHOW PROCESSLIST; +--------------------------------------+-----------------+-------+----------+------------------+ | id | host | state | database | extra_info | +--------------------------------------+-----------------+-------+----------+------------------+ | 1e6e5ed4-5441-43da-9ed6-eb6ba9baeb64 | 127.0.0.1:60080 | Query | default | show processlist | | 3d283add-4f60-416d-b9ca-662120614093 | 127.0.0.1:57018 | Query | default | NULL | +--------------------------------------+-----------------+-------+----------+------------------+","title":"SHOW PROCESSLIST"},{"location":"sqlstatement/show-commands/show-processlist/#syntax","text":"SHOW PROCESSLIST","title":"Syntax"},{"location":"sqlstatement/show-commands/show-processlist/#examples","text":"mysql> SHOW PROCESSLIST; +--------------------------------------+-----------------+-------+----------+------------------+ | id | host | state | database | extra_info | +--------------------------------------+-----------------+-------+----------+------------------+ | 1e6e5ed4-5441-43da-9ed6-eb6ba9baeb64 | 127.0.0.1:60080 | Query | default | show processlist | | 3d283add-4f60-416d-b9ca-662120614093 | 127.0.0.1:57018 | Query | default | NULL | +--------------------------------------+-----------------+-------+----------+------------------+","title":"Examples"},{"location":"sqlstatement/show-commands/show-tables/","text":"Shows the list of tables in the currently selected database. Syntax \u00b6 SHOW TABLES Examples \u00b6 mysql> USE system; mysql> SHOW TABLES; +----------+ | name | +----------+ | numbers | +----------+","title":"SHOW TABLES"},{"location":"sqlstatement/show-commands/show-tables/#syntax","text":"SHOW TABLES","title":"Syntax"},{"location":"sqlstatement/show-commands/show-tables/#examples","text":"mysql> USE system; mysql> SHOW TABLES; +----------+ | name | +----------+ | numbers | +----------+","title":"Examples"},{"location":"sqlstatement/string-functions/substring/","text":"SUBSTRING function is used to extract a string containing a specific number of characters from a particular position of a given string. Syntax \u00b6 SUBSTRING ( expression [ FROM position_expr ] [ FOR length_expr ]) Arguments \u00b6 Arguments Description expression The main string from where the character to be extracted position_expr The one-indexed position expression to start at. If negative, counts from the end length_expr The number expression of characters to extract Return Type \u00b6 String Note In SUBSTRING, the starting index point of a string is 1 (not 0). In the following example, the starting index 3 represents the third character in the string, because the index starts from 1. mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ Examples \u00b6 mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ mysql> SELECT SUBSTRING('1234567890' FROM 3); +------------------------------+ | SUBSTRING(1234567890,3,NULL) | +------------------------------+ | 34567890 | +------------------------------+ 1 row in set (0.01 sec) mysql> SELECT SUBSTRING('1234567890' FOR 3); +---------------------------+ | SUBSTRING(1234567890,1,3) | +---------------------------+ | 123 | +---------------------------+ 1 row in set (0.01 sec)","title":"SUBSTRING"},{"location":"sqlstatement/string-functions/substring/#syntax","text":"SUBSTRING ( expression [ FROM position_expr ] [ FOR length_expr ])","title":"Syntax"},{"location":"sqlstatement/string-functions/substring/#arguments","text":"Arguments Description expression The main string from where the character to be extracted position_expr The one-indexed position expression to start at. If negative, counts from the end length_expr The number expression of characters to extract","title":"Arguments"},{"location":"sqlstatement/string-functions/substring/#return-type","text":"String Note In SUBSTRING, the starting index point of a string is 1 (not 0). In the following example, the starting index 3 represents the third character in the string, because the index starts from 1. mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+","title":"Return Type"},{"location":"sqlstatement/string-functions/substring/#examples","text":"mysql> SELECT SUBSTRING('1234567890' FROM 3 FOR 3); +---------------------------+ | SUBSTRING(1234567890,3,3) | +---------------------------+ | 345 | +---------------------------+ mysql> SELECT SUBSTRING('1234567890' FROM 3); +------------------------------+ | SUBSTRING(1234567890,3,NULL) | +------------------------------+ | 34567890 | +------------------------------+ 1 row in set (0.01 sec) mysql> SELECT SUBSTRING('1234567890' FOR 3); +---------------------------+ | SUBSTRING(1234567890,1,3) | +---------------------------+ | 123 | +---------------------------+ 1 row in set (0.01 sec)","title":"Examples"},{"location":"sqlstatement/test-functions/crashme/","text":"CRASHME function makes a crash,. Warning Only used for testing where panic is required. This function is very useful for distributed query stability testing, we can trigger panic by hand. Currently CRASHME function there is no permission restrictions. Syntax \u00b6 crashme ( expression ) Arguments \u00b6 Arguments Description expression Any expression, like sleep(1) Return Type \u00b6 Null Examples \u00b6 mysql> SELECT * FROM (SELECT crashme(sleep(1)));; ERROR 2013 (HY000) at line 2: Lost connection to MySQL server during query","title":"CRASHME"},{"location":"sqlstatement/test-functions/crashme/#syntax","text":"crashme ( expression )","title":"Syntax"},{"location":"sqlstatement/test-functions/crashme/#arguments","text":"Arguments Description expression Any expression, like sleep(1)","title":"Arguments"},{"location":"sqlstatement/test-functions/crashme/#return-type","text":"Null","title":"Return Type"},{"location":"sqlstatement/test-functions/crashme/#examples","text":"mysql> SELECT * FROM (SELECT crashme(sleep(1)));; ERROR 2013 (HY000) at line 2: Lost connection to MySQL server during query","title":"Examples"},{"location":"sqlstatement/test-functions/sleep/","text":"Sleeps seconds seconds on each data block. Warning Only used for testing where sleep is required. Syntax \u00b6 sleep ( seconds ) Arguments \u00b6 Arguments Description seconds Must be a constant column of any nonnegative number or float.\uff5c Return Type \u00b6 UInt8 Examples \u00b6 mysql> SELECT sleep(2); +----------+ | sleep(2) | +----------+ | 0 | +----------+ 1 row in set (2.01 sec) mysql> SELECT sleep(2.7); +------------+ | sleep(2.7) | +------------+ | 0 | +------------+ 1 row in set (2.71 sec)","title":"SLEEP"},{"location":"sqlstatement/test-functions/sleep/#syntax","text":"sleep ( seconds )","title":"Syntax"},{"location":"sqlstatement/test-functions/sleep/#arguments","text":"Arguments Description seconds Must be a constant column of any nonnegative number or float.\uff5c","title":"Arguments"},{"location":"sqlstatement/test-functions/sleep/#return-type","text":"UInt8","title":"Return Type"},{"location":"sqlstatement/test-functions/sleep/#examples","text":"mysql> SELECT sleep(2); +----------+ | sleep(2) | +----------+ | 0 | +----------+ 1 row in set (2.01 sec) mysql> SELECT sleep(2.7); +------------+ | sleep(2.7) | +------------+ | 0 | +------------+ 1 row in set (2.71 sec)","title":"Examples"},{"location":"system/system-tables/","text":"Most system tables store their data in RAM. A DatafuseQuery server creates such system tables at the start. system.numbers \u00b6 This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are parallelized too. Used for tests. mysql> SELECT avg(number) FROM numbers(100000000); +-------------+ | avg(number) | +-------------+ | 49999999.5 | +-------------+ 1 row in set (0.04 sec) system.numbers_mt \u00b6 The same as system.numbers system.settings \u00b6 Contains information about session settings for current user. mysql> SELECT * FROM system.settings; +----------------+---------+---------------------------------------------------------------------------------------------------+ | name | value | description | +----------------+---------+---------------------------------------------------------------------------------------------------+ | max_block_size | 10000 | Maximum block size for reading | | max_threads | 8 | The maximum number of threads to execute the request. By default, it is determined automatically. | | default_db | default | The default database for current session | +----------------+---------+---------------------------------------------------------------------------------------------------+ 3 rows in set (0.00 sec) system.functions \u00b6 Contains information about normal and aggregate functions. mysql> SELECT * FROM system.functions limit 10; +----------+--------------+ | name | is_aggregate | +----------+--------------+ | + | false | | plus | false | | - | false | | minus | false | | * | false | | multiply | false | | / | false | | divide | false | | % | false | | modulo | false | +----------+--------------+ 10 rows in set (0.01 sec) system.contributors \u00b6 Contains information about contributors. The order is random at query execution time. mysql> SELECT * FROM system.contributors LIMIT 20; +-------------------------+ | name | +-------------------------+ | artorias1024 | | BohuTANG | | dependabot[bot] | | dependabot-preview[bot] | | drdr xp | | Eason | | hulunbier | | jyizheng | | leiysky | | smallfish | | sundy-li | | sundyli | | taiyang-li | | TLightSky | | Winter Zhang | | wubx | | yizheng | | Yizheng Jiao | | zhang2014 | | zhihanz | +-------------------------+ 20 rows in set (0.00 sec)","title":"System Tables"},{"location":"system/system-tables/#systemnumbers","text":"This table contains a single UInt64 column named number that contains almost all the natural numbers starting from zero. You can use this table for tests, or if you need to do a brute force search. Reads from this table are parallelized too. Used for tests. mysql> SELECT avg(number) FROM numbers(100000000); +-------------+ | avg(number) | +-------------+ | 49999999.5 | +-------------+ 1 row in set (0.04 sec)","title":"system.numbers"},{"location":"system/system-tables/#systemnumbers_mt","text":"The same as system.numbers","title":"system.numbers_mt"},{"location":"system/system-tables/#systemsettings","text":"Contains information about session settings for current user. mysql> SELECT * FROM system.settings; +----------------+---------+---------------------------------------------------------------------------------------------------+ | name | value | description | +----------------+---------+---------------------------------------------------------------------------------------------------+ | max_block_size | 10000 | Maximum block size for reading | | max_threads | 8 | The maximum number of threads to execute the request. By default, it is determined automatically. | | default_db | default | The default database for current session | +----------------+---------+---------------------------------------------------------------------------------------------------+ 3 rows in set (0.00 sec)","title":"system.settings"},{"location":"system/system-tables/#systemfunctions","text":"Contains information about normal and aggregate functions. mysql> SELECT * FROM system.functions limit 10; +----------+--------------+ | name | is_aggregate | +----------+--------------+ | + | false | | plus | false | | - | false | | minus | false | | * | false | | multiply | false | | / | false | | divide | false | | % | false | | modulo | false | +----------+--------------+ 10 rows in set (0.01 sec)","title":"system.functions"},{"location":"system/system-tables/#systemcontributors","text":"Contains information about contributors. The order is random at query execution time. mysql> SELECT * FROM system.contributors LIMIT 20; +-------------------------+ | name | +-------------------------+ | artorias1024 | | BohuTANG | | dependabot[bot] | | dependabot-preview[bot] | | drdr xp | | Eason | | hulunbier | | jyizheng | | leiysky | | smallfish | | sundy-li | | sundyli | | taiyang-li | | TLightSky | | Winter Zhang | | wubx | | yizheng | | Yizheng Jiao | | zhang2014 | | zhihanz | +-------------------------+ 20 rows in set (0.00 sec)","title":"system.contributors"}]}